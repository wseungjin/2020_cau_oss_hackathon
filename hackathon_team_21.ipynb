{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "hackathon_team_21",
      "provenance": [],
      "collapsed_sections": [
        "1AosAX9DXOlc"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wseungjin/2020_cau_oss_hackathon/blob/master/hackathon_team_21.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1AosAX9DXOlc"
      },
      "source": [
        "# **0. 해커톤 진행 주의사항**\n",
        "\n",
        "**1)  개발 관련 주의사항**\n",
        "*   [1. 초기 환경 설정]은 절대 수정하지 말 것\n",
        "*   모든 구현은 [2. 데이터 전처리] 및 [3.모델 생성]에서만 진행\n",
        "*   [4. 모델 저장]에서 team_name 변수 변경 (예.`team_name = 'team01'`)\n",
        " *    트레이닝 중간에 checkpoint를 활용하여 모델을 저장한 경우에도 파일 이름 양식 통일 필수\n",
        "*   Colab 사용중 실수로 데이터 손실이 발생할 수도 있으니 중간 결과값을 github에 업로드 \n",
        " *    \"런타임->모든 런타임 재설정\"은 절대 누르지 말 것 (저장한 모델 데이터가 모두 삭제됨)\n",
        "*   효율적인 구현 및 테스팅을 위해 GPU 가속 기능 활성화\n",
        " *    \"런타임 -> 런타임 유형변경 -> 하드웨어 가속기 -> GPU 설정\"\n",
        "*   주석을 최대한 자세히 작성\n",
        "*   Keras API 관련하여 [Keras Documentation](https://keras.io/) 참조\n",
        "\n",
        "**2) 제출 관련 주의사항**\n",
        "*  제출물\n",
        " *  소스코드 (hackathon_teamXX.ipynb)\n",
        " *  컴파일된 모델 파일 (model_entire_teamXX.h5)\n",
        " *  모델 발표 자료 \n",
        "* 제출 기한: **오후 5시 (단, 발표자료는 11시)**\n",
        "* 제출 방법: [GitHub README](https://github.com/cauosshackathonta/2020_cau_oss_hackathon/) 참조\n",
        "\n",
        " \n",
        "**3) 평가 관련 주의사항**\n",
        "*  모델 성능 = 테스트 데이터 셋 분류 정확도\n",
        " *  model.evaluate(x_test, y_test)\n",
        "*  제출된 모델들의 테스트 데이터 셋 분류 정확도를 기준으로 수상작 결정\n",
        "*  수상 후보들에 대해서는 소스코드를 기반으로 모델 재검증 \n",
        " \n",
        "**4) 수상 실격 사유**\n",
        "*  유사한 소스코드 or 알고리즘이 적발될 경우\n",
        "*  소스코드와 제출된 모델이 상이한 경우\n",
        "*  개발 관련 주의사항을 지키지 않은 경우\n",
        " *  예: [초기 환경 설정]을 수정한 경우\n",
        "*  데이터 셋을 변조한 경우\n",
        " *  예. 테스트 데이터 셋을 트레이닝 데이터 셋에 포함하여 모델 생성 \n",
        "*  주석이 소스코드와 맞지 않거나 미비할 경우\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "67lwEXhUqys1"
      },
      "source": [
        "# **1. 초기 환경 설정**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ms5PBBJ1qSC6",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals, unicode_literals\n",
        "\n",
        "# tensorflow와 tf.keras 및 관련 라이브러리 임포트\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras.utils import np_utils\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# 데이터셋 다운로드\n",
        "check = !if [ -d 'dataset/' ]; then echo \"1\" ; else echo \"0\"; fi\n",
        "if (check[0] is '0' ):\n",
        "  !mkdir dataset\n",
        "  !wget 'https://www.itl.nist.gov/iaui/vip/cs_links/EMNIST/matlab.zip'\n",
        "  !unzip matlab.zip -d /content/dataset\n",
        "\n",
        "# 데이터셋 로드\n",
        "from scipy import io as spio\n",
        "emnist = spio.loadmat(\"/content/dataset/matlab/emnist-balanced.mat\")\n",
        "\n",
        "x_train = emnist[\"dataset\"][0][0][0][0][0][0]\n",
        "y_train = emnist[\"dataset\"][0][0][0][0][0][1]\n",
        "\n",
        "x_test = emnist[\"dataset\"][0][0][1][0][0][0]\n",
        "y_test = emnist[\"dataset\"][0][0][1][0][0][1]\n",
        "\n",
        "# # 분류를 위해 클래스 벡터를 바이너리 매트릭스로 변환\n",
        "y_train = np_utils.to_categorical(y_train)\n",
        "y_test = np_utils.to_categorical(y_test)\n",
        "\n",
        "# 데이터 28x28 이미지화\n",
        "x_train = x_train.reshape(x_train.shape[0], 28, 28, 1).astype('float32')\n",
        "x_test = x_test.reshape(x_test.shape[0], 28, 28, 1).astype('float32')\n",
        "\n",
        "# 총 클래스 개수\n",
        "num_classes = y_test.shape[1]\n",
        "input_shape = x_test.shape[1:]"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "A-YjppJpXBO9"
      },
      "source": [
        "# **2. 데이터 전처리**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-o32ARzbX1WM",
        "colab_type": "text"
      },
      "source": [
        "### Normalization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "QZ9KWTBP6AI1",
        "colab": {}
      },
      "source": [
        "# 기본적인 normalization\n",
        "x_train_after = x_train / 255.0\n",
        "x_test_after = x_test / 255.0"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F1kdNNWzX-3u",
        "colab_type": "text"
      },
      "source": [
        "### image augmentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yXHRrb5FzdaJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "efe02e0c-dd94-46cb-f0ad-19ba25dd5368"
      },
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# 새로운 이미지들을 생성\n",
        "# rotatation range 10내의 각도 변화 가능\n",
        "# zomm 0.05 가능\n",
        "# weight, height shift 0.05 가능\n",
        "# flip 하지않음\n",
        "# whitening 시킴\n",
        "image_generator = ImageDataGenerator(\n",
        "            rotation_range=10,\n",
        "            zoom_range = 0.05, \n",
        "            width_shift_range=0.05,\n",
        "            height_shift_range=0.05,\n",
        "            horizontal_flip=False,\n",
        "            vertical_flip=False, \n",
        "            data_format=\"channels_last\",\n",
        "            zca_whitening=True)\n",
        "\n",
        "image_generator.fit(x_train_after, augment=True)\n",
        "\n",
        "# 새로 조건을 넣어서 생성할 이미지 개수 \n",
        "augment_size = 100000\n",
        "\n",
        "# 트레인 사이즈\n",
        "train_size = x_train_after.shape[0]\n",
        "\n",
        "# 이미지 생성 \n",
        "randidx = np.random.randint(train_size, size=augment_size)\n",
        "x_augmented = x_train_after[randidx].copy()\n",
        "y_augmented = y_train[randidx].copy()\n",
        "x_augmented = image_generator.flow(x_augmented, np.zeros(augment_size),\n",
        "                            batch_size=augment_size, shuffle=False).next()[0]\n",
        "\n",
        "# 기존 데이터 + 추가된 이미 합침\n",
        "x_train_after = np.concatenate((x_train_after, x_augmented))\n",
        "y_train = np.concatenate((y_train, y_augmented))\n",
        "\n",
        "# 트레인 사이즈\n",
        "train_size = x_train_after.shape[0]"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/image_data_generator.py:337: UserWarning: This ImageDataGenerator specifies `zca_whitening`, which overrides setting of `featurewise_center`.\n",
            "  warnings.warn('This ImageDataGenerator specifies '\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "v-lo-O1yiFpY"
      },
      "source": [
        "# **3. 모델 생성**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xROH1OYvYIYJ",
        "colab_type": "text"
      },
      "source": [
        "### Model structure"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DZP4eRmRqgRp",
        "colab": {}
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Conv2D, MaxPooling2D, Dropout, Flatten, LeakyReLU, Activation\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "\n",
        "\n",
        "# 순차 모델 생성 (가장 기본구조)\n",
        "model = Sequential()\n",
        "\n",
        "\n",
        "# 1st hidden layer: CNN layer\n",
        "# 초기 학습을 빠르게 하기 위하여 glorot_uniform으로 값을 초기화 한다.\n",
        "# CNN 과정 중에 shrink되기 때문에 패딩을 same으로 해준다.\n",
        "model.add(Conv2D(32, kernel_size=(3, 3),  padding='same', input_shape=input_shape, kernel_initializer='glorot_uniform'))\n",
        "# gradient를 스무스하게 하기 위하여 BatchNormalization을 사용하였다.\n",
        "model.add(BatchNormalization())\n",
        "# gradient vanishment를 피하기 위하여 LeakyReLU를 사용하였다.\n",
        "model.add(LeakyReLU(alpha=0.01))\n",
        "\n",
        "# 2st hidden layer: CNN layer\n",
        "model.add(Conv2D(32, (3,3), padding='same', kernel_initializer='glorot_uniform'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(LeakyReLU(alpha=0.01))\n",
        "\n",
        "# overfitting 문제를 해결하기 위하여 Dropout 기법을 사용하였다.\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "# 3st hidden layer: CNN layer\n",
        "model.add(Conv2D(64, kernel_size=(3, 3),  padding='same', kernel_initializer='glorot_uniform'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(LeakyReLU(alpha=0.01))\n",
        "\n",
        "# 4st hidden layer: CNN layer\n",
        "model.add(Conv2D(64, (3,3),  padding='same', kernel_initializer='glorot_uniform'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(LeakyReLU(alpha=0.01))      \n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "# 5st hidden layer: CNN layer\n",
        "model.add(Conv2D(64, kernel_size=(3, 3),  padding='same', kernel_initializer='glorot_uniform'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(LeakyReLU(alpha=0.01))\n",
        "\n",
        "# 6st hidden layer: CNN layer\n",
        "model.add(Conv2D(64, (3,3),  padding='same', kernel_initializer='glorot_uniform'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(LeakyReLU(alpha=0.01))      \n",
        "\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "# 7st hidden layer: CNN layer\n",
        "model.add(Conv2D(64, kernel_size=(3, 3),  padding='same', kernel_initializer='glorot_uniform'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(LeakyReLU(alpha=0.01))\n",
        "\n",
        "# 8st hidden layer: CNN layer\n",
        "model.add(Conv2D(64, (3,3),  padding='same', kernel_initializer='glorot_uniform'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(LeakyReLU(alpha=0.01))    \n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "# 9st hidden layer: CNN layer\n",
        "model.add(Conv2D(128, kernel_size=(3, 3),  padding='same', kernel_initializer='glorot_uniform'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(LeakyReLU(alpha=0.01))\n",
        "\n",
        "# 10st hidden layer: CNN layer\n",
        "model.add(Conv2D(128, (3,3),  padding='same', kernel_initializer='glorot_uniform'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(LeakyReLU(alpha=0.01))          \n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "# Flatten layer: fully connected layer에 사용하기 위하여 1D vector로 만든다.\n",
        "model.add(Flatten())\n",
        "          \n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "# Output layer: fully-connected layer를 이용하여 output값을 classify한다.\n",
        "model.add(Dense(num_classes))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "# 모델 컴파일\n",
        "# optimizer: 모델을 업데이트 하는 방식\n",
        "# loss: 모델의 정확도를 판단하는 방식\n",
        "# metrics: 트레이닝 및 테스팅 성능 모니터링을 위한 평가지표\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dbo4El7W5YGk",
        "colab_type": "text"
      },
      "source": [
        "### 모델 요약"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k0TJMDKk5T0h",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "07bbec23-6ed9-49dc-b510-2b78a6934389"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 28, 28, 32)        320       \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 28, 28, 32)        128       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu (LeakyReLU)      (None, 28, 28, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 28, 28, 32)        9248      \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 28, 28, 32)        128       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_1 (LeakyReLU)    (None, 28, 28, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 28, 28, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 28, 28, 64)        18496     \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 28, 28, 64)        256       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_2 (LeakyReLU)    (None, 28, 28, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 28, 28, 64)        36928     \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 28, 28, 64)        256       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_3 (LeakyReLU)    (None, 28, 28, 64)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 14, 14, 64)        0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 14, 14, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 14, 14, 64)        36928     \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 14, 14, 64)        256       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_4 (LeakyReLU)    (None, 14, 14, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 14, 14, 64)        36928     \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 14, 14, 64)        256       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_5 (LeakyReLU)    (None, 14, 14, 64)        0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 14, 14, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_6 (Conv2D)            (None, 14, 14, 64)        36928     \n",
            "_________________________________________________________________\n",
            "batch_normalization_6 (Batch (None, 14, 14, 64)        256       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_6 (LeakyReLU)    (None, 14, 14, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_7 (Conv2D)            (None, 14, 14, 64)        36928     \n",
            "_________________________________________________________________\n",
            "batch_normalization_7 (Batch (None, 14, 14, 64)        256       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_7 (LeakyReLU)    (None, 14, 14, 64)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 7, 7, 64)          0         \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 7, 7, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_8 (Conv2D)            (None, 7, 7, 128)         73856     \n",
            "_________________________________________________________________\n",
            "batch_normalization_8 (Batch (None, 7, 7, 128)         512       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_8 (LeakyReLU)    (None, 7, 7, 128)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_9 (Conv2D)            (None, 7, 7, 128)         147584    \n",
            "_________________________________________________________________\n",
            "batch_normalization_9 (Batch (None, 7, 7, 128)         512       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_9 (LeakyReLU)    (None, 7, 7, 128)         0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 3, 3, 128)         0         \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 3, 3, 128)         0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 1152)              0         \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 1152)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 47)                54191     \n",
            "_________________________________________________________________\n",
            "batch_normalization_10 (Batc (None, 47)                188       \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 47)                0         \n",
            "=================================================================\n",
            "Total params: 491,339\n",
            "Trainable params: 489,837\n",
            "Non-trainable params: 1,502\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dX7WCaAaYTJ0",
        "colab_type": "text"
      },
      "source": [
        "### 체크포인트"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ieS202I30pk6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 체크포인트 생성\n",
        "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath='/content/checkpoint_entire_best.h5', monitor='val_accuracy', verbose=1, save_weight_only=False, save_best_only=True, mode='auto')"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bi-bmpcqYVli",
        "colab_type": "text"
      },
      "source": [
        "### 학습"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2uOPH0z_0qYE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "2da43a3d-d64e-420d-dbe8-24f666eccd2f"
      },
      "source": [
        "# 모델 트레이닝\n",
        "# batch_size: 전체 데이터셋 중 몇개씩 학습시킬 것인지# shuffle: 학습전에 트레이닝 데이터셋을 랜덤하게 섞을 것인지\n",
        "# validation_data: 중간 성능 검증에 사용할 data set\n",
        "history = model.fit(x_train_after, y_train, batch_size = 128, epochs = 100, shuffle=True, callbacks=[cp_callback], validation_data=(x_test_after, y_test))\n",
        "# epoch: 학습에 전체 데이터셋이 총 몇번 이용될 것인지\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "1663/1663 [==============================] - ETA: 0s - loss: 1.0125 - accuracy: 0.7484\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.87133, saving model to /content/checkpoint_entire_best.h5\n",
            "1663/1663 [==============================] - 62s 37ms/step - loss: 1.0125 - accuracy: 0.7484 - val_loss: 0.3818 - val_accuracy: 0.8713\n",
            "Epoch 2/100\n",
            "1663/1663 [==============================] - ETA: 0s - loss: 0.4658 - accuracy: 0.8544\n",
            "Epoch 00002: val_accuracy improved from 0.87133 to 0.87936, saving model to /content/checkpoint_entire_best.h5\n",
            "1663/1663 [==============================] - 61s 37ms/step - loss: 0.4658 - accuracy: 0.8544 - val_loss: 0.3404 - val_accuracy: 0.8794\n",
            "Epoch 3/100\n",
            "1663/1663 [==============================] - ETA: 0s - loss: 0.4045 - accuracy: 0.8668\n",
            "Epoch 00003: val_accuracy improved from 0.87936 to 0.89160, saving model to /content/checkpoint_entire_best.h5\n",
            "1663/1663 [==============================] - 61s 37ms/step - loss: 0.4045 - accuracy: 0.8668 - val_loss: 0.3042 - val_accuracy: 0.8916\n",
            "Epoch 4/100\n",
            "1663/1663 [==============================] - ETA: 0s - loss: 0.3707 - accuracy: 0.8748\n",
            "Epoch 00004: val_accuracy improved from 0.89160 to 0.89170, saving model to /content/checkpoint_entire_best.h5\n",
            "1663/1663 [==============================] - 61s 37ms/step - loss: 0.3707 - accuracy: 0.8748 - val_loss: 0.3010 - val_accuracy: 0.8917\n",
            "Epoch 5/100\n",
            "1663/1663 [==============================] - ETA: 0s - loss: 0.3504 - accuracy: 0.8788\n",
            "Epoch 00005: val_accuracy improved from 0.89170 to 0.89181, saving model to /content/checkpoint_entire_best.h5\n",
            "1663/1663 [==============================] - 61s 37ms/step - loss: 0.3504 - accuracy: 0.8788 - val_loss: 0.3044 - val_accuracy: 0.8918\n",
            "Epoch 6/100\n",
            "1663/1663 [==============================] - ETA: 0s - loss: 0.3331 - accuracy: 0.8836\n",
            "Epoch 00006: val_accuracy improved from 0.89181 to 0.89340, saving model to /content/checkpoint_entire_best.h5\n",
            "1663/1663 [==============================] - 61s 37ms/step - loss: 0.3331 - accuracy: 0.8836 - val_loss: 0.3024 - val_accuracy: 0.8934\n",
            "Epoch 7/100\n",
            "1663/1663 [==============================] - ETA: 0s - loss: 0.3195 - accuracy: 0.8871\n",
            "Epoch 00007: val_accuracy improved from 0.89340 to 0.89777, saving model to /content/checkpoint_entire_best.h5\n",
            "1663/1663 [==============================] - 61s 37ms/step - loss: 0.3195 - accuracy: 0.8871 - val_loss: 0.2852 - val_accuracy: 0.8978\n",
            "Epoch 8/100\n",
            "1663/1663 [==============================] - ETA: 0s - loss: 0.3093 - accuracy: 0.8896\n",
            "Epoch 00008: val_accuracy did not improve from 0.89777\n",
            "1663/1663 [==============================] - 61s 37ms/step - loss: 0.3093 - accuracy: 0.8896 - val_loss: 0.2833 - val_accuracy: 0.8970\n",
            "Epoch 9/100\n",
            "1663/1663 [==============================] - ETA: 0s - loss: 0.2992 - accuracy: 0.8925\n",
            "Epoch 00009: val_accuracy improved from 0.89777 to 0.90027, saving model to /content/checkpoint_entire_best.h5\n",
            "1663/1663 [==============================] - 61s 37ms/step - loss: 0.2992 - accuracy: 0.8925 - val_loss: 0.2798 - val_accuracy: 0.9003\n",
            "Epoch 10/100\n",
            "1663/1663 [==============================] - ETA: 0s - loss: 0.2907 - accuracy: 0.8956\n",
            "Epoch 00010: val_accuracy improved from 0.90027 to 0.90128, saving model to /content/checkpoint_entire_best.h5\n",
            "1663/1663 [==============================] - 61s 37ms/step - loss: 0.2907 - accuracy: 0.8956 - val_loss: 0.2791 - val_accuracy: 0.9013\n",
            "Epoch 11/100\n",
            "1663/1663 [==============================] - ETA: 0s - loss: 0.2839 - accuracy: 0.8965\n",
            "Epoch 00011: val_accuracy improved from 0.90128 to 0.90245, saving model to /content/checkpoint_entire_best.h5\n",
            "1663/1663 [==============================] - 61s 37ms/step - loss: 0.2839 - accuracy: 0.8965 - val_loss: 0.2705 - val_accuracy: 0.9024\n",
            "Epoch 12/100\n",
            "1663/1663 [==============================] - ETA: 0s - loss: 0.2765 - accuracy: 0.8988\n",
            "Epoch 00012: val_accuracy did not improve from 0.90245\n",
            "1663/1663 [==============================] - 61s 37ms/step - loss: 0.2765 - accuracy: 0.8988 - val_loss: 0.2787 - val_accuracy: 0.9015\n",
            "Epoch 13/100\n",
            "1663/1663 [==============================] - ETA: 0s - loss: 0.2714 - accuracy: 0.9007\n",
            "Epoch 00013: val_accuracy did not improve from 0.90245\n",
            "1663/1663 [==============================] - 61s 37ms/step - loss: 0.2714 - accuracy: 0.9007 - val_loss: 0.2792 - val_accuracy: 0.9022\n",
            "Epoch 14/100\n",
            "1663/1663 [==============================] - ETA: 0s - loss: 0.2668 - accuracy: 0.9015\n",
            "Epoch 00014: val_accuracy improved from 0.90245 to 0.90500, saving model to /content/checkpoint_entire_best.h5\n",
            "1663/1663 [==============================] - 61s 37ms/step - loss: 0.2668 - accuracy: 0.9015 - val_loss: 0.2706 - val_accuracy: 0.9050\n",
            "Epoch 15/100\n",
            "1663/1663 [==============================] - ETA: 0s - loss: 0.2613 - accuracy: 0.9032\n",
            "Epoch 00015: val_accuracy did not improve from 0.90500\n",
            "1663/1663 [==============================] - 61s 37ms/step - loss: 0.2613 - accuracy: 0.9032 - val_loss: 0.2702 - val_accuracy: 0.9039\n",
            "Epoch 16/100\n",
            "1663/1663 [==============================] - ETA: 0s - loss: 0.2580 - accuracy: 0.9043\n",
            "Epoch 00016: val_accuracy did not improve from 0.90500\n",
            "1663/1663 [==============================] - 61s 37ms/step - loss: 0.2580 - accuracy: 0.9043 - val_loss: 0.2751 - val_accuracy: 0.9030\n",
            "Epoch 17/100\n",
            "1663/1663 [==============================] - ETA: 0s - loss: 0.2528 - accuracy: 0.9059\n",
            "Epoch 00017: val_accuracy did not improve from 0.90500\n",
            "1663/1663 [==============================] - 61s 37ms/step - loss: 0.2528 - accuracy: 0.9059 - val_loss: 0.2704 - val_accuracy: 0.9032\n",
            "Epoch 18/100\n",
            "1663/1663 [==============================] - ETA: 0s - loss: 0.2495 - accuracy: 0.9062\n",
            "Epoch 00018: val_accuracy improved from 0.90500 to 0.90569, saving model to /content/checkpoint_entire_best.h5\n",
            "1663/1663 [==============================] - 61s 37ms/step - loss: 0.2495 - accuracy: 0.9062 - val_loss: 0.2697 - val_accuracy: 0.9057\n",
            "Epoch 19/100\n",
            "1663/1663 [==============================] - ETA: 0s - loss: 0.2454 - accuracy: 0.9080\n",
            "Epoch 00019: val_accuracy improved from 0.90569 to 0.90617, saving model to /content/checkpoint_entire_best.h5\n",
            "1663/1663 [==============================] - 61s 37ms/step - loss: 0.2454 - accuracy: 0.9080 - val_loss: 0.2680 - val_accuracy: 0.9062\n",
            "Epoch 20/100\n",
            "1663/1663 [==============================] - ETA: 0s - loss: 0.2425 - accuracy: 0.9089\n",
            "Epoch 00020: val_accuracy did not improve from 0.90617\n",
            "1663/1663 [==============================] - 61s 37ms/step - loss: 0.2425 - accuracy: 0.9089 - val_loss: 0.2713 - val_accuracy: 0.9055\n",
            "Epoch 21/100\n",
            "1663/1663 [==============================] - ETA: 0s - loss: 0.2397 - accuracy: 0.9088\n",
            "Epoch 00021: val_accuracy did not improve from 0.90617\n",
            "1663/1663 [==============================] - 61s 37ms/step - loss: 0.2397 - accuracy: 0.9088 - val_loss: 0.2744 - val_accuracy: 0.9046\n",
            "Epoch 22/100\n",
            "1663/1663 [==============================] - ETA: 0s - loss: 0.2357 - accuracy: 0.9101\n",
            "Epoch 00022: val_accuracy did not improve from 0.90617\n",
            "1663/1663 [==============================] - 61s 37ms/step - loss: 0.2357 - accuracy: 0.9101 - val_loss: 0.2699 - val_accuracy: 0.9050\n",
            "Epoch 23/100\n",
            "1663/1663 [==============================] - ETA: 0s - loss: 0.2332 - accuracy: 0.9112\n",
            "Epoch 00023: val_accuracy did not improve from 0.90617\n",
            "1663/1663 [==============================] - 61s 37ms/step - loss: 0.2332 - accuracy: 0.9112 - val_loss: 0.2724 - val_accuracy: 0.9046\n",
            "Epoch 24/100\n",
            "1663/1663 [==============================] - ETA: 0s - loss: 0.2308 - accuracy: 0.9117\n",
            "Epoch 00024: val_accuracy did not improve from 0.90617\n",
            "1663/1663 [==============================] - 61s 37ms/step - loss: 0.2308 - accuracy: 0.9117 - val_loss: 0.2768 - val_accuracy: 0.9058\n",
            "Epoch 25/100\n",
            "1663/1663 [==============================] - ETA: 0s - loss: 0.2291 - accuracy: 0.9125\n",
            "Epoch 00025: val_accuracy did not improve from 0.90617\n",
            "1663/1663 [==============================] - 61s 37ms/step - loss: 0.2291 - accuracy: 0.9125 - val_loss: 0.2724 - val_accuracy: 0.9034\n",
            "Epoch 26/100\n",
            "1663/1663 [==============================] - ETA: 0s - loss: 0.2277 - accuracy: 0.9133\n",
            "Epoch 00026: val_accuracy did not improve from 0.90617\n",
            "1663/1663 [==============================] - 61s 37ms/step - loss: 0.2277 - accuracy: 0.9133 - val_loss: 0.2683 - val_accuracy: 0.9053\n",
            "Epoch 27/100\n",
            "1663/1663 [==============================] - ETA: 0s - loss: 0.2239 - accuracy: 0.9140\n",
            "Epoch 00027: val_accuracy did not improve from 0.90617\n",
            "1663/1663 [==============================] - 61s 37ms/step - loss: 0.2239 - accuracy: 0.9140 - val_loss: 0.2740 - val_accuracy: 0.9049\n",
            "Epoch 28/100\n",
            "1663/1663 [==============================] - ETA: 0s - loss: 0.2235 - accuracy: 0.9140\n",
            "Epoch 00028: val_accuracy did not improve from 0.90617\n",
            "1663/1663 [==============================] - 61s 37ms/step - loss: 0.2235 - accuracy: 0.9140 - val_loss: 0.2739 - val_accuracy: 0.9061\n",
            "Epoch 29/100\n",
            "1663/1663 [==============================] - ETA: 0s - loss: 0.2204 - accuracy: 0.9145\n",
            "Epoch 00029: val_accuracy did not improve from 0.90617\n",
            "1663/1663 [==============================] - 61s 37ms/step - loss: 0.2204 - accuracy: 0.9145 - val_loss: 0.2758 - val_accuracy: 0.9061\n",
            "Epoch 30/100\n",
            "1663/1663 [==============================] - ETA: 0s - loss: 0.2195 - accuracy: 0.9152\n",
            "Epoch 00030: val_accuracy improved from 0.90617 to 0.90681, saving model to /content/checkpoint_entire_best.h5\n",
            "1663/1663 [==============================] - 61s 37ms/step - loss: 0.2195 - accuracy: 0.9152 - val_loss: 0.2755 - val_accuracy: 0.9068\n",
            "Epoch 31/100\n",
            "1663/1663 [==============================] - ETA: 0s - loss: 0.2167 - accuracy: 0.9163\n",
            "Epoch 00031: val_accuracy did not improve from 0.90681\n",
            "1663/1663 [==============================] - 61s 37ms/step - loss: 0.2167 - accuracy: 0.9163 - val_loss: 0.2732 - val_accuracy: 0.9045\n",
            "Epoch 32/100\n",
            "1663/1663 [==============================] - ETA: 0s - loss: 0.2145 - accuracy: 0.9175\n",
            "Epoch 00032: val_accuracy did not improve from 0.90681\n",
            "1663/1663 [==============================] - 61s 37ms/step - loss: 0.2145 - accuracy: 0.9175 - val_loss: 0.2803 - val_accuracy: 0.9060\n",
            "Epoch 33/100\n",
            "1663/1663 [==============================] - ETA: 0s - loss: 0.2129 - accuracy: 0.9174\n",
            "Epoch 00033: val_accuracy did not improve from 0.90681\n",
            "1663/1663 [==============================] - 61s 37ms/step - loss: 0.2129 - accuracy: 0.9174 - val_loss: 0.2743 - val_accuracy: 0.9060\n",
            "Epoch 34/100\n",
            "1663/1663 [==============================] - ETA: 0s - loss: 0.2109 - accuracy: 0.9182\n",
            "Epoch 00034: val_accuracy did not improve from 0.90681\n",
            "1663/1663 [==============================] - 61s 37ms/step - loss: 0.2109 - accuracy: 0.9182 - val_loss: 0.2788 - val_accuracy: 0.9059\n",
            "Epoch 35/100\n",
            "1663/1663 [==============================] - ETA: 0s - loss: 0.2093 - accuracy: 0.9184\n",
            "Epoch 00035: val_accuracy improved from 0.90681 to 0.90691, saving model to /content/checkpoint_entire_best.h5\n",
            "1663/1663 [==============================] - 61s 37ms/step - loss: 0.2093 - accuracy: 0.9184 - val_loss: 0.2777 - val_accuracy: 0.9069\n",
            "Epoch 36/100\n",
            "1663/1663 [==============================] - ETA: 0s - loss: 0.2075 - accuracy: 0.9186\n",
            "Epoch 00036: val_accuracy improved from 0.90691 to 0.90713, saving model to /content/checkpoint_entire_best.h5\n",
            "1663/1663 [==============================] - 61s 37ms/step - loss: 0.2075 - accuracy: 0.9186 - val_loss: 0.2816 - val_accuracy: 0.9071\n",
            "Epoch 37/100\n",
            "1663/1663 [==============================] - ETA: 0s - loss: 0.2075 - accuracy: 0.9190\n",
            "Epoch 00037: val_accuracy improved from 0.90713 to 0.90782, saving model to /content/checkpoint_entire_best.h5\n",
            "1663/1663 [==============================] - 61s 37ms/step - loss: 0.2075 - accuracy: 0.9190 - val_loss: 0.2769 - val_accuracy: 0.9078\n",
            "Epoch 38/100\n",
            "1663/1663 [==============================] - ETA: 0s - loss: 0.2050 - accuracy: 0.9197\n",
            "Epoch 00038: val_accuracy did not improve from 0.90782\n",
            "1663/1663 [==============================] - 61s 37ms/step - loss: 0.2050 - accuracy: 0.9197 - val_loss: 0.2812 - val_accuracy: 0.9054\n",
            "Epoch 39/100\n",
            "1663/1663 [==============================] - ETA: 0s - loss: 0.2037 - accuracy: 0.9203\n",
            "Epoch 00039: val_accuracy did not improve from 0.90782\n",
            "1663/1663 [==============================] - 61s 37ms/step - loss: 0.2037 - accuracy: 0.9203 - val_loss: 0.2800 - val_accuracy: 0.9076\n",
            "Epoch 40/100\n",
            "1663/1663 [==============================] - ETA: 0s - loss: 0.2021 - accuracy: 0.9211\n",
            "Epoch 00040: val_accuracy did not improve from 0.90782\n",
            "1663/1663 [==============================] - 61s 37ms/step - loss: 0.2021 - accuracy: 0.9211 - val_loss: 0.2727 - val_accuracy: 0.9063\n",
            "Epoch 41/100\n",
            "1663/1663 [==============================] - ETA: 0s - loss: 0.2010 - accuracy: 0.9215\n",
            "Epoch 00041: val_accuracy did not improve from 0.90782\n",
            "1663/1663 [==============================] - 61s 37ms/step - loss: 0.2010 - accuracy: 0.9215 - val_loss: 0.2828 - val_accuracy: 0.9055\n",
            "Epoch 42/100\n",
            "1663/1663 [==============================] - ETA: 0s - loss: 0.2004 - accuracy: 0.9218\n",
            "Epoch 00042: val_accuracy did not improve from 0.90782\n",
            "1663/1663 [==============================] - 61s 37ms/step - loss: 0.2004 - accuracy: 0.9218 - val_loss: 0.2854 - val_accuracy: 0.9052\n",
            "Epoch 43/100\n",
            "1663/1663 [==============================] - ETA: 0s - loss: 0.1987 - accuracy: 0.9225\n",
            "Epoch 00043: val_accuracy did not improve from 0.90782\n",
            "1663/1663 [==============================] - 61s 37ms/step - loss: 0.1987 - accuracy: 0.9225 - val_loss: 0.2829 - val_accuracy: 0.9061\n",
            "Epoch 44/100\n",
            "1663/1663 [==============================] - ETA: 0s - loss: 0.1986 - accuracy: 0.9219\n",
            "Epoch 00044: val_accuracy did not improve from 0.90782\n",
            "1663/1663 [==============================] - 61s 37ms/step - loss: 0.1986 - accuracy: 0.9219 - val_loss: 0.2816 - val_accuracy: 0.9051\n",
            "Epoch 45/100\n",
            "1663/1663 [==============================] - ETA: 0s - loss: 0.1961 - accuracy: 0.9231\n",
            "Epoch 00045: val_accuracy did not improve from 0.90782\n",
            "1663/1663 [==============================] - 61s 37ms/step - loss: 0.1961 - accuracy: 0.9231 - val_loss: 0.2840 - val_accuracy: 0.9046\n",
            "Epoch 46/100\n",
            "1663/1663 [==============================] - ETA: 0s - loss: 0.1950 - accuracy: 0.9232\n",
            "Epoch 00046: val_accuracy did not improve from 0.90782\n",
            "1663/1663 [==============================] - 61s 37ms/step - loss: 0.1950 - accuracy: 0.9232 - val_loss: 0.2884 - val_accuracy: 0.9070\n",
            "Epoch 47/100\n",
            "1663/1663 [==============================] - ETA: 0s - loss: 0.1943 - accuracy: 0.9235\n",
            "Epoch 00047: val_accuracy did not improve from 0.90782\n",
            "1663/1663 [==============================] - 61s 37ms/step - loss: 0.1943 - accuracy: 0.9235 - val_loss: 0.2844 - val_accuracy: 0.9049\n",
            "Epoch 48/100\n",
            "1663/1663 [==============================] - ETA: 0s - loss: 0.1925 - accuracy: 0.9240\n",
            "Epoch 00048: val_accuracy did not improve from 0.90782\n",
            "1663/1663 [==============================] - 61s 37ms/step - loss: 0.1925 - accuracy: 0.9240 - val_loss: 0.2910 - val_accuracy: 0.9061\n",
            "Epoch 49/100\n",
            "1663/1663 [==============================] - ETA: 0s - loss: 0.1917 - accuracy: 0.9242\n",
            "Epoch 00049: val_accuracy did not improve from 0.90782\n",
            "1663/1663 [==============================] - 61s 37ms/step - loss: 0.1917 - accuracy: 0.9242 - val_loss: 0.2884 - val_accuracy: 0.9053\n",
            "Epoch 50/100\n",
            "1663/1663 [==============================] - ETA: 0s - loss: 0.1898 - accuracy: 0.9256\n",
            "Epoch 00050: val_accuracy did not improve from 0.90782\n",
            "1663/1663 [==============================] - 62s 37ms/step - loss: 0.1898 - accuracy: 0.9256 - val_loss: 0.2822 - val_accuracy: 0.9070\n",
            "Epoch 51/100\n",
            "1663/1663 [==============================] - ETA: 0s - loss: 0.1896 - accuracy: 0.9248\n",
            "Epoch 00051: val_accuracy did not improve from 0.90782\n",
            "1663/1663 [==============================] - 62s 37ms/step - loss: 0.1896 - accuracy: 0.9248 - val_loss: 0.2907 - val_accuracy: 0.9064\n",
            "Epoch 52/100\n",
            "1663/1663 [==============================] - ETA: 0s - loss: 0.1879 - accuracy: 0.9254\n",
            "Epoch 00052: val_accuracy did not improve from 0.90782\n",
            "1663/1663 [==============================] - 62s 37ms/step - loss: 0.1879 - accuracy: 0.9254 - val_loss: 0.2820 - val_accuracy: 0.9063\n",
            "Epoch 53/100\n",
            "1663/1663 [==============================] - ETA: 0s - loss: 0.1867 - accuracy: 0.9264\n",
            "Epoch 00053: val_accuracy did not improve from 0.90782\n",
            "1663/1663 [==============================] - 62s 37ms/step - loss: 0.1867 - accuracy: 0.9264 - val_loss: 0.2884 - val_accuracy: 0.9074\n",
            "Epoch 54/100\n",
            "1663/1663 [==============================] - ETA: 0s - loss: 0.1847 - accuracy: 0.9268\n",
            "Epoch 00054: val_accuracy did not improve from 0.90782\n",
            "1663/1663 [==============================] - 62s 37ms/step - loss: 0.1847 - accuracy: 0.9268 - val_loss: 0.2865 - val_accuracy: 0.9074\n",
            "Epoch 55/100\n",
            "1663/1663 [==============================] - ETA: 0s - loss: 0.1849 - accuracy: 0.9270\n",
            "Epoch 00055: val_accuracy did not improve from 0.90782\n",
            "1663/1663 [==============================] - 62s 37ms/step - loss: 0.1849 - accuracy: 0.9270 - val_loss: 0.2867 - val_accuracy: 0.9052\n",
            "Epoch 56/100\n",
            "1663/1663 [==============================] - ETA: 0s - loss: 0.1856 - accuracy: 0.9258\n",
            "Epoch 00056: val_accuracy did not improve from 0.90782\n",
            "1663/1663 [==============================] - 62s 37ms/step - loss: 0.1856 - accuracy: 0.9258 - val_loss: 0.2901 - val_accuracy: 0.9074\n",
            "Epoch 57/100\n",
            "1663/1663 [==============================] - ETA: 0s - loss: 0.1840 - accuracy: 0.9272\n",
            "Epoch 00057: val_accuracy did not improve from 0.90782\n",
            "1663/1663 [==============================] - 62s 37ms/step - loss: 0.1840 - accuracy: 0.9272 - val_loss: 0.2924 - val_accuracy: 0.9065\n",
            "Epoch 58/100\n",
            "1663/1663 [==============================] - ETA: 0s - loss: 0.1824 - accuracy: 0.9278\n",
            "Epoch 00058: val_accuracy did not improve from 0.90782\n",
            "1663/1663 [==============================] - 62s 37ms/step - loss: 0.1824 - accuracy: 0.9278 - val_loss: 0.2917 - val_accuracy: 0.9054\n",
            "Epoch 59/100\n",
            "1662/1663 [============================>.] - ETA: 0s - loss: 0.1807 - accuracy: 0.9282\n",
            "Epoch 00059: val_accuracy did not improve from 0.90782\n",
            "1663/1663 [==============================] - 62s 37ms/step - loss: 0.1806 - accuracy: 0.9282 - val_loss: 0.2954 - val_accuracy: 0.9054\n",
            "Epoch 60/100\n",
            "1663/1663 [==============================] - ETA: 0s - loss: 0.1802 - accuracy: 0.9276\n",
            "Epoch 00060: val_accuracy did not improve from 0.90782\n",
            "1663/1663 [==============================] - 62s 37ms/step - loss: 0.1802 - accuracy: 0.9276 - val_loss: 0.2893 - val_accuracy: 0.9074\n",
            "Epoch 61/100\n",
            "1663/1663 [==============================] - ETA: 0s - loss: 0.1798 - accuracy: 0.9282\n",
            "Epoch 00061: val_accuracy did not improve from 0.90782\n",
            "1663/1663 [==============================] - 62s 37ms/step - loss: 0.1798 - accuracy: 0.9282 - val_loss: 0.2990 - val_accuracy: 0.9054\n",
            "Epoch 62/100\n",
            "1663/1663 [==============================] - ETA: 0s - loss: 0.1791 - accuracy: 0.9291\n",
            "Epoch 00062: val_accuracy did not improve from 0.90782\n",
            "1663/1663 [==============================] - 62s 37ms/step - loss: 0.1791 - accuracy: 0.9291 - val_loss: 0.2934 - val_accuracy: 0.9049\n",
            "Epoch 63/100\n",
            "1663/1663 [==============================] - ETA: 0s - loss: 0.1796 - accuracy: 0.9284\n",
            "Epoch 00063: val_accuracy did not improve from 0.90782\n",
            "1663/1663 [==============================] - 62s 37ms/step - loss: 0.1796 - accuracy: 0.9284 - val_loss: 0.2919 - val_accuracy: 0.9061\n",
            "Epoch 64/100\n",
            "1663/1663 [==============================] - ETA: 0s - loss: 0.1766 - accuracy: 0.9293\n",
            "Epoch 00064: val_accuracy did not improve from 0.90782\n",
            "1663/1663 [==============================] - 62s 37ms/step - loss: 0.1766 - accuracy: 0.9293 - val_loss: 0.2926 - val_accuracy: 0.9063\n",
            "Epoch 65/100\n",
            "1662/1663 [============================>.] - ETA: 0s - loss: 0.1762 - accuracy: 0.9301\n",
            "Epoch 00065: val_accuracy did not improve from 0.90782\n",
            "1663/1663 [==============================] - 62s 37ms/step - loss: 0.1761 - accuracy: 0.9301 - val_loss: 0.2994 - val_accuracy: 0.9049\n",
            "Epoch 66/100\n",
            "1663/1663 [==============================] - ETA: 0s - loss: 0.1759 - accuracy: 0.9302\n",
            "Epoch 00066: val_accuracy did not improve from 0.90782\n",
            "1663/1663 [==============================] - 62s 37ms/step - loss: 0.1759 - accuracy: 0.9302 - val_loss: 0.3007 - val_accuracy: 0.9060\n",
            "Epoch 67/100\n",
            "1663/1663 [==============================] - ETA: 0s - loss: 0.1749 - accuracy: 0.9307\n",
            "Epoch 00067: val_accuracy did not improve from 0.90782\n",
            "1663/1663 [==============================] - 62s 37ms/step - loss: 0.1749 - accuracy: 0.9307 - val_loss: 0.2982 - val_accuracy: 0.9075\n",
            "Epoch 68/100\n",
            "1663/1663 [==============================] - ETA: 0s - loss: 0.1748 - accuracy: 0.9301\n",
            "Epoch 00068: val_accuracy improved from 0.90782 to 0.90840, saving model to /content/checkpoint_entire_best.h5\n",
            "1663/1663 [==============================] - 62s 37ms/step - loss: 0.1748 - accuracy: 0.9301 - val_loss: 0.2989 - val_accuracy: 0.9084\n",
            "Epoch 69/100\n",
            "1662/1663 [============================>.] - ETA: 0s - loss: 0.1743 - accuracy: 0.9306\n",
            "Epoch 00069: val_accuracy did not improve from 0.90840\n",
            "1663/1663 [==============================] - 62s 37ms/step - loss: 0.1743 - accuracy: 0.9306 - val_loss: 0.2934 - val_accuracy: 0.9078\n",
            "Epoch 70/100\n",
            "1663/1663 [==============================] - ETA: 0s - loss: 0.1732 - accuracy: 0.9308\n",
            "Epoch 00070: val_accuracy did not improve from 0.90840\n",
            "1663/1663 [==============================] - 62s 37ms/step - loss: 0.1732 - accuracy: 0.9308 - val_loss: 0.2967 - val_accuracy: 0.9071\n",
            "Epoch 71/100\n",
            "1663/1663 [==============================] - ETA: 0s - loss: 0.1713 - accuracy: 0.9315\n",
            "Epoch 00071: val_accuracy did not improve from 0.90840\n",
            "1663/1663 [==============================] - 62s 37ms/step - loss: 0.1713 - accuracy: 0.9315 - val_loss: 0.3039 - val_accuracy: 0.9038\n",
            "Epoch 72/100\n",
            "1663/1663 [==============================] - ETA: 0s - loss: 0.1710 - accuracy: 0.9318\n",
            "Epoch 00072: val_accuracy did not improve from 0.90840\n",
            "1663/1663 [==============================] - 62s 37ms/step - loss: 0.1710 - accuracy: 0.9318 - val_loss: 0.2963 - val_accuracy: 0.9075\n",
            "Epoch 73/100\n",
            "1663/1663 [==============================] - ETA: 0s - loss: 0.1711 - accuracy: 0.9314\n",
            "Epoch 00073: val_accuracy did not improve from 0.90840\n",
            "1663/1663 [==============================] - 62s 37ms/step - loss: 0.1711 - accuracy: 0.9314 - val_loss: 0.2966 - val_accuracy: 0.9051\n",
            "Epoch 74/100\n",
            "1663/1663 [==============================] - ETA: 0s - loss: 0.1705 - accuracy: 0.9316\n",
            "Epoch 00074: val_accuracy did not improve from 0.90840\n",
            "1663/1663 [==============================] - 62s 37ms/step - loss: 0.1705 - accuracy: 0.9316 - val_loss: 0.2988 - val_accuracy: 0.9059\n",
            "Epoch 75/100\n",
            "1662/1663 [============================>.] - ETA: 0s - loss: 0.1710 - accuracy: 0.9309\n",
            "Epoch 00075: val_accuracy did not improve from 0.90840\n",
            "1663/1663 [==============================] - 62s 37ms/step - loss: 0.1709 - accuracy: 0.9310 - val_loss: 0.2995 - val_accuracy: 0.9082\n",
            "Epoch 76/100\n",
            "1663/1663 [==============================] - ETA: 0s - loss: 0.1689 - accuracy: 0.9325\n",
            "Epoch 00076: val_accuracy did not improve from 0.90840\n",
            "1663/1663 [==============================] - 62s 37ms/step - loss: 0.1689 - accuracy: 0.9325 - val_loss: 0.2913 - val_accuracy: 0.9073\n",
            "Epoch 77/100\n",
            "1663/1663 [==============================] - ETA: 0s - loss: 0.1677 - accuracy: 0.9328\n",
            "Epoch 00077: val_accuracy did not improve from 0.90840\n",
            "1663/1663 [==============================] - 62s 37ms/step - loss: 0.1677 - accuracy: 0.9328 - val_loss: 0.3023 - val_accuracy: 0.9061\n",
            "Epoch 78/100\n",
            "1663/1663 [==============================] - ETA: 0s - loss: 0.1682 - accuracy: 0.9326\n",
            "Epoch 00078: val_accuracy did not improve from 0.90840\n",
            "1663/1663 [==============================] - 62s 38ms/step - loss: 0.1682 - accuracy: 0.9326 - val_loss: 0.2985 - val_accuracy: 0.9060\n",
            "Epoch 79/100\n",
            "1663/1663 [==============================] - ETA: 0s - loss: 0.1667 - accuracy: 0.9335\n",
            "Epoch 00079: val_accuracy did not improve from 0.90840\n",
            "1663/1663 [==============================] - 62s 37ms/step - loss: 0.1667 - accuracy: 0.9335 - val_loss: 0.2999 - val_accuracy: 0.9083\n",
            "Epoch 80/100\n",
            "1663/1663 [==============================] - ETA: 0s - loss: 0.1672 - accuracy: 0.9330\n",
            "Epoch 00080: val_accuracy did not improve from 0.90840\n",
            "1663/1663 [==============================] - 62s 37ms/step - loss: 0.1672 - accuracy: 0.9330 - val_loss: 0.3032 - val_accuracy: 0.9056\n",
            "Epoch 81/100\n",
            "1663/1663 [==============================] - ETA: 0s - loss: 0.1660 - accuracy: 0.9339\n",
            "Epoch 00081: val_accuracy did not improve from 0.90840\n",
            "1663/1663 [==============================] - 62s 37ms/step - loss: 0.1660 - accuracy: 0.9339 - val_loss: 0.3029 - val_accuracy: 0.9055\n",
            "Epoch 82/100\n",
            "1663/1663 [==============================] - ETA: 0s - loss: 0.1644 - accuracy: 0.9342\n",
            "Epoch 00082: val_accuracy did not improve from 0.90840\n",
            "1663/1663 [==============================] - 62s 37ms/step - loss: 0.1644 - accuracy: 0.9342 - val_loss: 0.3052 - val_accuracy: 0.9069\n",
            "Epoch 83/100\n",
            "1663/1663 [==============================] - ETA: 0s - loss: 0.1644 - accuracy: 0.9342\n",
            "Epoch 00083: val_accuracy did not improve from 0.90840\n",
            "1663/1663 [==============================] - 62s 37ms/step - loss: 0.1644 - accuracy: 0.9342 - val_loss: 0.3066 - val_accuracy: 0.9064\n",
            "Epoch 84/100\n",
            "1663/1663 [==============================] - ETA: 0s - loss: 0.1646 - accuracy: 0.9340\n",
            "Epoch 00084: val_accuracy did not improve from 0.90840\n",
            "1663/1663 [==============================] - 62s 37ms/step - loss: 0.1646 - accuracy: 0.9340 - val_loss: 0.3033 - val_accuracy: 0.9047\n",
            "Epoch 85/100\n",
            "1663/1663 [==============================] - ETA: 0s - loss: 0.1626 - accuracy: 0.9352\n",
            "Epoch 00085: val_accuracy did not improve from 0.90840\n",
            "1663/1663 [==============================] - 62s 37ms/step - loss: 0.1626 - accuracy: 0.9352 - val_loss: 0.3104 - val_accuracy: 0.9062\n",
            "Epoch 86/100\n",
            "1662/1663 [============================>.] - ETA: 0s - loss: 0.1647 - accuracy: 0.9343\n",
            "Epoch 00086: val_accuracy did not improve from 0.90840\n",
            "1663/1663 [==============================] - 62s 37ms/step - loss: 0.1647 - accuracy: 0.9343 - val_loss: 0.3017 - val_accuracy: 0.9057\n",
            "Epoch 87/100\n",
            "1663/1663 [==============================] - ETA: 0s - loss: 0.1614 - accuracy: 0.9348\n",
            "Epoch 00087: val_accuracy did not improve from 0.90840\n",
            "1663/1663 [==============================] - 62s 37ms/step - loss: 0.1614 - accuracy: 0.9348 - val_loss: 0.3071 - val_accuracy: 0.9053\n",
            "Epoch 88/100\n",
            "1663/1663 [==============================] - ETA: 0s - loss: 0.1616 - accuracy: 0.9349\n",
            "Epoch 00088: val_accuracy did not improve from 0.90840\n",
            "1663/1663 [==============================] - 62s 37ms/step - loss: 0.1616 - accuracy: 0.9349 - val_loss: 0.3045 - val_accuracy: 0.9060\n",
            "Epoch 89/100\n",
            "1663/1663 [==============================] - ETA: 0s - loss: 0.1610 - accuracy: 0.9354\n",
            "Epoch 00089: val_accuracy did not improve from 0.90840\n",
            "1663/1663 [==============================] - 62s 37ms/step - loss: 0.1610 - accuracy: 0.9354 - val_loss: 0.3047 - val_accuracy: 0.9066\n",
            "Epoch 90/100\n",
            "1663/1663 [==============================] - ETA: 0s - loss: 0.1602 - accuracy: 0.9353\n",
            "Epoch 00090: val_accuracy did not improve from 0.90840\n",
            "1663/1663 [==============================] - 62s 37ms/step - loss: 0.1602 - accuracy: 0.9353 - val_loss: 0.3101 - val_accuracy: 0.9056\n",
            "Epoch 91/100\n",
            "1663/1663 [==============================] - ETA: 0s - loss: 0.1610 - accuracy: 0.9352\n",
            "Epoch 00091: val_accuracy did not improve from 0.90840\n",
            "1663/1663 [==============================] - 62s 37ms/step - loss: 0.1610 - accuracy: 0.9352 - val_loss: 0.3122 - val_accuracy: 0.9048\n",
            "Epoch 92/100\n",
            "1663/1663 [==============================] - ETA: 0s - loss: 0.1599 - accuracy: 0.9365\n",
            "Epoch 00092: val_accuracy did not improve from 0.90840\n",
            "1663/1663 [==============================] - 62s 37ms/step - loss: 0.1599 - accuracy: 0.9365 - val_loss: 0.3047 - val_accuracy: 0.9059\n",
            "Epoch 93/100\n",
            "1663/1663 [==============================] - ETA: 0s - loss: 0.1585 - accuracy: 0.9360\n",
            "Epoch 00093: val_accuracy did not improve from 0.90840\n",
            "1663/1663 [==============================] - 62s 37ms/step - loss: 0.1585 - accuracy: 0.9360 - val_loss: 0.3044 - val_accuracy: 0.9068\n",
            "Epoch 94/100\n",
            "1663/1663 [==============================] - ETA: 0s - loss: 0.1594 - accuracy: 0.9360\n",
            "Epoch 00094: val_accuracy did not improve from 0.90840\n",
            "1663/1663 [==============================] - 62s 37ms/step - loss: 0.1594 - accuracy: 0.9360 - val_loss: 0.3098 - val_accuracy: 0.9056\n",
            "Epoch 95/100\n",
            "1663/1663 [==============================] - ETA: 0s - loss: 0.1584 - accuracy: 0.9362\n",
            "Epoch 00095: val_accuracy did not improve from 0.90840\n",
            "1663/1663 [==============================] - 62s 37ms/step - loss: 0.1584 - accuracy: 0.9362 - val_loss: 0.3047 - val_accuracy: 0.9070\n",
            "Epoch 96/100\n",
            "1663/1663 [==============================] - ETA: 0s - loss: 0.1587 - accuracy: 0.9365\n",
            "Epoch 00096: val_accuracy did not improve from 0.90840\n",
            "1663/1663 [==============================] - 62s 37ms/step - loss: 0.1587 - accuracy: 0.9365 - val_loss: 0.3099 - val_accuracy: 0.9065\n",
            "Epoch 97/100\n",
            "1663/1663 [==============================] - ETA: 0s - loss: 0.1571 - accuracy: 0.9372\n",
            "Epoch 00097: val_accuracy did not improve from 0.90840\n",
            "1663/1663 [==============================] - 62s 37ms/step - loss: 0.1571 - accuracy: 0.9372 - val_loss: 0.3194 - val_accuracy: 0.9053\n",
            "Epoch 98/100\n",
            "1663/1663 [==============================] - ETA: 0s - loss: 0.1565 - accuracy: 0.9369\n",
            "Epoch 00098: val_accuracy did not improve from 0.90840\n",
            "1663/1663 [==============================] - 62s 37ms/step - loss: 0.1565 - accuracy: 0.9369 - val_loss: 0.3152 - val_accuracy: 0.9053\n",
            "Epoch 99/100\n",
            "1663/1663 [==============================] - ETA: 0s - loss: 0.1553 - accuracy: 0.9377\n",
            "Epoch 00099: val_accuracy did not improve from 0.90840\n",
            "1663/1663 [==============================] - 62s 37ms/step - loss: 0.1553 - accuracy: 0.9377 - val_loss: 0.3101 - val_accuracy: 0.9057\n",
            "Epoch 100/100\n",
            "1662/1663 [============================>.] - ETA: 0s - loss: 0.1557 - accuracy: 0.9378\n",
            "Epoch 00100: val_accuracy did not improve from 0.90840\n",
            "1663/1663 [==============================] - 62s 37ms/step - loss: 0.1557 - accuracy: 0.9378 - val_loss: 0.3174 - val_accuracy: 0.9049\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0bk6muKLYfaP",
        "colab_type": "text"
      },
      "source": [
        "### 러닝 커브 출력"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kqCI2gTYWRCC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 573
        },
        "outputId": "be5b67b3-3907-4152-928a-6c38a376eb77"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# summarize history for accuracy\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()\n",
        "# summarize history for loss\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAEWCAYAAABxMXBSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3dd3hc1Zn48e+rXq1uY0sucsEFG9wwpiWYkhgwYJNAKCYhYeNkgYSwJAGyQAibZJP9ZTekEBJaqKaEUJxgeky3seUCuFuuktxkq1h9NDPv749zZY+KpZHRWLL0fp5nHs3ceq7uzHnvOefec0RVMcYYY8IV1d0JMMYYc2yxwGGMMaZTLHAYY4zpFAscxhhjOsUChzHGmE6xwGGMMaZTLHAY0w4ReVREfh7msttE5NxIp8mY7maBwxhjTKdY4DCmDxCRmO5Og+k9LHCYY55XRfQjEflURGpE5GERGSAir4pIlYi8JSIZIctfLCJrRKRCRN4RkbEh8yaJyApvvWeBhBb7miUiq7x1PxKRE8NM44UislJEDohIkYjc3WL+Gd72Krz513rTE0Xkf0Vku4hUisgH3rSzRKS4jf/Dud77u0XkeRF5UkQOANeKyDQRWeztY5eI/FFE4kLWP0FE3hSRMhHZIyI/EZHjRKRWRLJClpssIqUiEhvOsZvexwKH6S2+ApwHHA9cBLwK/ATIwX3Pvw8gIscDTwM/8OYtBP4hInFeJvoS8ASQCfzN2y7eupOAR4DvAFnAX4AFIhIfRvpqgK8D6cCFwL+LyGxvu0O99P7BS9NEYJW33m+AKcBpXpp+DATD/J9cAjzv7fMpIADcDGQDpwLnANd7aUgF3gJeAwYBI4G3VXU38A5wech2rwGeUdXGMNNhehkLHKa3+IOq7lHVEuB94GNVXamq9cCLwCRvua8Br6jqm17G9xsgEZcxTwdigXtVtVFVnweWhexjHvAXVf1YVQOq+hjQ4K3XLlV9R1U/U9Wgqn6KC15f9GZfBbylqk97+92vqqtEJAr4FnCTqpZ4+/xIVRvC/J8sVtWXvH3WqepyVV2iqn5V3YYLfE1pmAXsVtX/VdV6Va1S1Y+9eY8BcwFEJBq4EhdcTR9lgcP0FntC3te18TnFez8I2N40Q1WDQBGQ680r0eY9f24PeT8UuMWr6qkQkQpgsLdeu0TkFBFZ5FXxVALfxV35421jcxurZeOqytqaF46iFmk4XkT+KSK7veqrX4aRBoCXgXEiko8r1VWq6tIjTJPpBSxwmL5mJy4AACAigss0S4BdQK43rcmQkPdFwC9UNT3klaSqT4ex3/nAAmCwqqYBfwaa9lMEjGhjnX1A/WHm1QBJIccRjavmCtWy6+v7gfXAKFXth6vKC03D8LYS7pXansOVOq7BSht9ngUO09c8B1woIud4jbu34KqbPgIWA37g+yISKyKXAtNC1n0Q+K5XehARSfYavVPD2G8qUKaq9SIyDVc91eQp4FwRuVxEYkQkS0QmeqWhR4D/E5FBIhItIqd6bSobgQRv/7HAHUBHbS2pwAGgWkTGAP8eMu+fwEAR+YGIxItIqoicEjL/ceBa4GIscPR5FjhMn6KqG3BXzn/AXdFfBFykqj5V9QGX4jLIMlx7yAsh6xYA3wb+CJQDhd6y4bgeuEdEqoC7cAGsabs7gAtwQawM1zB+kjf7h8BnuLaWMuDXQJSqVnrbfAhXWqoBmt1l1YYf4gJWFS4IPhuShipcNdRFwG5gEzAjZP6HuEb5FaoaWn1n+iCxgZyMMeEQkX8B81X1oe5Oi+leFjiMMR0SkZOBN3FtNFXdnR7TvayqyhjTLhF5DPeMxw8saBiwEocxxphOshKHMcaYTukTHZ9lZ2frsGHDujsZxhhzTFm+fPk+VW35fFDfCBzDhg2joKCgu5NhjDHHFBFp89Zrq6oyxhjTKRY4jDHGdIoFDmOMMZ3SJ9o42tLY2EhxcTH19fXdnZSISkhIIC8vj9hYG3PHGNM1+mzgKC4uJjU1lWHDhtG8M9TeQ1XZv38/xcXF5Ofnd3dyjDG9RJ+tqqqvrycrK6vXBg0AESErK6vXl6qMMUdXnw0cQK8OGk36wjEaY46uPltVZYwxvYmqUri3mrW7DnCg3k9Ng5/qej/XnZFPRnJcl+7LAkc3qaioYP78+Vx//fWdWu+CCy5g/vz5pKenRyhlxpijobKukegoISW+42w4EFQqan1s2lvN0q1lLNtWRklFHdkp8fRPjccfUJZtK2N/ja/ZelECl0wcZIGjt6ioqOBPf/pTq8Dh9/uJiTn8aVm4cGGkk2aMiaCNe6p44L0tvLyqhMaAMigtgRH9U8hMjiNKhCgR6hr97K/2UVbjXuW1PoJef7QiMHpAKsf3T6WsxseanQcIBJUvHp/DKcMzmTg4g4zkWFLiY0iMjY5IdbUFjm5y2223sXnzZiZOnEhsbCwJCQlkZGSwfv16Nm7cyOzZsykqKqK+vp6bbrqJefPmAYe6T6murub888/njDPO4KOPPiI3N5eXX36ZxMTEbj4yY3qP7ftreOyj7azZWckXR+dw4YSBDM1KpqbBz7pdB9hcWk2tL0B9Y5CaBj87ymrZuq+GovJaAGKjo4iLjiIxLprkuGgAPimuJCE2iiunDWFAvwQK91azaW8VRWW1BFQJBiEhNoqs5HhG5KQwdVgcOSlxZKXEk5eRyNShmaQlde/t9X2iW/WpU6dqy76q1q1bx9ixYwH42T/WsHbngS7d57hB/fjpRSccdv62bduYNWsWq1ev5p133uHCCy9k9erVB2+bLSsrIzMzk7q6Ok4++WTeffddsrKymgWOkSNHUlBQwMSJE7n88su5+OKLmTt3bqt9hR6rMX1ZnS/Ah4X7WLfrACUVdZRU1FFV7yc2WoiOEhJjo0lPiiM9KZYd+2v514a9RIswIieFDXvcUCQD0xLYfaCelllnlEBuRiL52SkMyUwkWgRfQPH5g9Q3Bqjx+anzBThtRDbXnDqUzC6uPooEEVmuqlNbTrcSRw8xbdq0Zs9a/P73v+fFF18EoKioiE2bNpGVldVsnfz8fCZOnAjAlClT2LZt21FLrzHdKRBUKusaqaj1HWwrSIyNJiY6ik+LK3h3QynvF+4jNkoYOSCVkTkpFJXX8v6mUuobgwBkp8STm5FIv4QYAkHFH1BKqxvYtLeaitpGEuOi+d6MkVw9fSgD+iVQVFbLq6t38WlxJSP7pzB+UBqjj0slNSGG+Jho4mOiiIrqG3cxWuCAdksGR0tycvLB9++88w5vvfUWixcvJikpibPOOqvNZzHi4+MPvo+Ojqauru6opNWYrqCq1PoClNX4aPAHGJiWSHIHDcVV9Y08+N4WHv5gKzW+wGGXy0iK5cxROURHCZv2VjF/636ykuP52tTBnDtuACcPyyQhNrpT6R2cmcS8L4zo1Dq9lQWObpKamkpVVdujcFZWVpKRkUFSUhLr169nyZIlRzl1xnROfWOAKBHiYlo/Gqaq1PgCVNY1sqW0msWb9/PR5v2s23WABn+w2bIZSbHkpMZT1xigut6Pzx9k5IBUTsxNIzM5jieWbKesxseFEwZy8rAM0pPiSEuMJahKXWOAOl+AUQNSmZCbRnTI1b+q2jNNXcgCRzfJysri9NNPZ/z48SQmJjJgwICD82bOnMmf//xnxo4dy+jRo5k+fXo3ptQYZ9u+Gt7bVMr+ah9BVfxBpbi8jnW7DrCltJqY6ChOGNSPk/LSSYyLZtOeKjbtraakvA5/8FCDQEyUcNLgdK6ZPpSc1HgykuKIi4liV2U9xeW17KtuICkuhpT4GKKjhHW7DvDiyhKqG/ycNiKL284fw4l5nbsd3YJG17LG8T6gLx2rOTKFe6tZvHkfI3JSmDw0g4TYaBoDQQq2lfOv9Xt4e/1etpTWHFxeBKJFGNAvgbED+zFuYCr1/iCrdlTwWUkl/mCQ/OxkRg1IZUhmEumJsaQnxTIwLZEpQzM6rJJqKRhU9tf4yE6JsyBwFFnjuDF9SDCo7CirZdv+GnaU1VJZ28j4vDQmD8kgLTGWyrpGPiuuZNm2Ml5bvfvgHUMA8TFRjM9NY9OeKg7U+4mLjuKU4Zl8ffpQZozpz5DMpHYzb38giOJuRe0qUVFCTmp8xwuaoyKigUNEZgK/A6KBh1T1Vy3mDwUeAXKAMmCuqhaLyETgfqAfEAB+oarPeus8CnwRqPQ2c62qrorkcRjTE9Q3BiitamBvVT1ZyfEMy05uNn9zaTWvfraLgu3lLN9eTlW9v9U2RGBAqrudtOnzyUMzufuicXxxdH+2lFbzYeF+VuwoZ+b44zh7zADOHJXdqRJCTBcGDNMzRSxwiEg0cB9wHlAMLBORBaq6NmSx3wCPq+pjInI28N/ANUAt8HVV3SQig4DlIvK6qlZ46/1IVZ+PVNqN6W6Fe6t4fc0e1u48QFF5LUVltZTXNjZb5tThWcydPpSM5Fgefn8rb6/fC8Co/inMOnEQkwank5+TzNDMJJLjY/ikqIKC7eVsKa1m1IBUTsxL48Tc9GYPk+VnJ3PO2AEY055IljimAYWqugVARJ4BLgFCA8c44D+894uAlwBUdWPTAqq6U0T24kolFRhzDFNVFm/ez9+WF5OTGs8p+ZlMHZbJgbpGVpdU8klxJW+v28OmvdUADMtKYnBmEuMnDGRQWgL9UxPISY1n3e4DPLVkBzfMXwFAZnIcN50ziqunD6F/akKb+z5tZDanjcw+asdqeq9IBo5coCjkczFwSotlPgEuxVVnzQFSRSRLVfc3LSAi04A4YHPIer8QkbuAt4HbVLUhAuk35nNRVfZWNRzsa6iorJbHF29nzc4D9EuIob4xyAPvbWm2TkyUMHVYBnOnn8CXTziO49LaDgIzxvTnO18YwXsbS6mo83H++IGdfi7BmCPV3Y3jPwT+KCLXAu8BJbg2DQBEZCDwBPANVW264ft2YDcumDwA3Arc03LDIjIPmAcwZMiQyB2B6ZMCQWXrvhpqGvzU+gJUN/gprWqgtKqBXZV1bNxTxaY91VQ1NG9nGJGTzK8uncDsSbkArNhRzort5aQnxTEh1z2JHG4AiI4SZozp3+XHZkxHIhk4SoDBIZ/zvGkHqepOXIkDEUkBvtLUjiEi/YBXgP9U1SUh6+zy3jaIyF9xwacVVX0AF1iYOnVqj7vn+Ei7VQe49957mTdvHklJSRFImQEXGDaXVrNqRwUH6huZkJvG+Nw0GgNBniso4okl2ykqa/tJ/azkOEb2T2HO5FxG9k8hJyWe9KQ4MpPjGNU/pVm3FKeNyOa0EVZ9ZI4tkQwcy4BRIpKPCxhXAFeFLiAi2UCZV5q4HXeHFSISB7yIazh/vsU6A1V1l7j7AWcDqyN4DBFzuG7Vw3Hvvfcyd+5cCxyfkz8QpKi8jsK91Wwuraa4vJbdle6upS2lNVS3KC1ECcREReELBJmWn8mNM0aSnRJPYlw0KfExZKfEk50S3+bT08b0JhELHKrqF5Ebgddxt+M+oqprROQeoEBVFwBnAf8tIoqrqrrBW/1y4AtAlleNBYduu31KRHIAAVYB343UMURSaLfq5513Hv379+e5556joaGBOXPm8LOf/Yyamhouv/xyiouLCQQC3HnnnezZs4edO3cyY8YMsrOzWbRoUXcfyjGjzhdgzc5Klm0rZ8mW/RRsK2vW31F6UizH9Uugf78E5kxKZ+LgdE4anE5aYiyflVSwakcF1Q0Bvjolj3GD+nXjkRjTvezJcYBXb4Pdn3XtTo+bAOf/6rCzQ7tVf+ONN3j++ef5y1/+gqpy8cUX8+Mf/5jS0lJee+01HnzwQcD1YZWWlnawa/Xs7PCqOPrqk+M7K+r4eOt+Pt5SxqqiCjbtrSbgdX0xqn8K04dncWJeGiP7pzA8J4W0xO4d48CYnsaeHO/B3njjDd544w0mTZoEQHV1NZs2beLMM8/klltu4dZbb2XWrFmceeaZ3ZzSnqGi1kd5bSMH6hqprGukvNZHeY2PstpGSsrrKC6vZUdZLbsq3UNu/RJimDw0gy+NG8CEPFeSsKeQjTlyFjig3ZLB0aCq3H777XznO99pNW/FihUsXLiQO+64g3POOYe77rqrG1J49FXVN/JpcSUxUUJiXDR1vgDvbSrl7XV7Wb+77V6Fm56KHpyZyPThWUzITWP68CzGHJfaZ8ZJMOZosMDRTUK7Vf/yl7/MnXfeydVXX01KSgolJSXExsbi9/vJzMxk7ty5pKen89BDDzVbN9yqqmOFqrJ8eznPLCvilU93UdfYfLyF6Cjh5GEZ3DpzDAPTEuiXGENqQiwZSXFkJMWSlhhr3V0YcxRY4Ogmod2qn3/++Vx11VWceuqpAKSkpPDkk09SWFjIj370I6KiooiNjeX+++8HYN68ecycOZNBgwb1isbxorJaXlxZwgsritm2v5bkuGhmTxrE+eMHEh0l1HkN2CcP6/6xlo0x1jjeTSk6unrSsaoqpVUNfFJcyeLN+1m8xQ3oAzB9eCaXTs7jwgkDO93ttjGm61njuDlqKmp9LNlSxuLN+9h9oJ5AEILqxlPYsvfQ09TxMVFMGZrBj2eO5uKTBpGXYc+lGHMssMBhPreyGh9Lt+7n461lLN1axtpdB1CFxNhohmQmERUlREdBv4RY5kzOZUROCmOOS2XikHTiY6x/JWOONX06cPSFcYgjURV5oL6R9buq+KBwH+9u2MunJZWoQkJsFJOHZPCDc47n9JFZnJiXbk9RG9ML9dnAkZCQwP79+8nKyuq1wUNV2b9/PwkJbfewGq46X4A31+3hn5/sZM3OA5RUuD6aRGDi4HR+cM7xnDEqiwm5FiiM6Qv6bODIy8ujuLiY0tLS7k5KRCUkJJCXl3dE635WXMlji7fx6me7qPEFGJiWwMnDMrnqlCGMHpDKlKEZZCTHdW2CjTE9Xp8NHLGxseTn53d3MnqEvVX1rNhegQjERUdRWdfI/I93sHRbGclx0Vx44kDmTMrjlPxMe5DOGNN3A0dfV17jY/7SHby5dg+riloPrDg4M5E7Z43jsql59Es4xp6dCAahYjukDYZo+4ofNdV7YeNrsOE18FXBwJNg4ETInQwZ+a5u80hVFsOS+yFnNIy+EJKz2l/eVwOLfgmDJsGErx75fk2b7FfVx6gqf19Rwi9eWUt5bSMn5aVxy3nHc8aobOJjomkMuPGyxuemEX00SheNdRAMQHxK59f1+6CkAKp2Q10ZVJdCyXIoWgoNlZA3Db72JKR6Y2jv+gQWfB+iY2Had2DcJRDTBVVtDdWwYzGMOBuiwrhLLBiEzf+CuGTXGWbLYw80woZXYdVTUBEyiObQU+G8/4K4pEPLvff/oGQFHP9lGDML+g1sf98b34AVj8EZN0Neq9vzj8z2j+DdX8OWdwGFtCEuY//4AQh4g3OmDIAh093/fPxXmq+/dgEs+RNc8P/c/6PV9hfDc9dAzT63ffkBDDsDJs31zmGLfsf2FcKzc6F0nft8oAROv6nttFfthgM7IXsUxKd+nv/C51NXATEJENuiPXLXJy5oZh/vgm8PuRDqsw8A9hWNgSDrd1Wx+0A9uw/U8/aqrZRuX815OeVcOS6BAaOmuivChLTObTgYCC+TBFB1GfqeNRD0u3Urd8COJbBzlcvIL7kPxl8akvA6KF7mlm0SFeNevmpY9w9YtwDqypvvK2eMy6DSBsP7/wsJ6XDFky5zff0nkJQFsUlQthlSjoOTrnCZbt601j/KYMCtV18J+We2zqAAKnbA/Ctg7xoYdiZc+gD0G+Tm1VXAtvchZyxkjXBX3CXL4ZUfws4V3gbEZQr9BrmMKzYRNi+Cmr3QL9ddMQMEfLDpTeg/Fi57zAWb57/lAla/PDhQ7LY15FSYeCWcMKd5RthQBa//pwsaEg0onPY9OOsnrTMrXy1seh3Kt0NNKdSWQf4XYMJlh/5HqrD9Qxe4trwDyf3h5Otc8BpwgjvWQCPsXeeC+44lsO1Dl85Z98LUb7rt7PoUHv4S+OvceZn9J5d2cAF2xWOw8EeQPgSufNp9L9YtgNUvQPlWSMqGyddA9mj3Paotg7fvce/n/Bk+eQbWvOACx7k/O1Tq2bcJPrwXPnkWgo1uWr88GHoazPgJZIZUY2//CLa+5zLunOPd+YpLbv4/qy1z53bHYnesu1dD3hQXJMfMgsT01t+d/Zvd93jDq1D0sfsNTpoLU7/lguR7/wOFbx1aPjoOcqfAyf/mAmZ0GzUBqu5720UB5nAPAFrgOJZV7YHG2uZfck8gqLy0soR7395IUVkdUQS5NeZp/i1mIdG0POfiMqTjZ8K4i131gqq7UqssgsRM98ONSYCt78LyR2HDQhhxDlz6l0NBJxiAT552V3HpQyF9sLtiWv4o7F3bfJfRcTBossvkt38ExUvdj/uLt8LKJ+H9/4Pq3Yc/9thkGHMBjJvtMuXEDPcKzdx3fwZPX+WOAYVRX4LZ97vjKXwLlv7FZXpBvzuGQZNcBpjS31W7FL7lSjIA8WkwdhaMudBljGlD3I/92bkug5z2bVeVEhMHZ98BxcthzYsuQwS3fM5ot82U/nDu3S4du1a5zLOm1GXuvmp31T35GzDy3OYZQOHb8MK3obHeZfaN9XDR7+DEy2DvepehfvY32LfRZcLDz3LnDFzmXVkMp33fBYy373GZcuYIGH0+DBjvgte6BfDpc9BwwPs/J7lX7T7IGObOUW0ZrJrvgm9Stiu9TP3WoZLQ4QQa4ekrYfPbcMV8F6wfPAsCfrjqWXjlFvc9mHSNuyDY9gHUV7j/w1cebp75BoOwZREsexg2vgoHR5bGZa6XPea+f8EALPwhFDziAnFMgrvg2bfJfVcmXeMuCvZtgtL1sP4V93049Ub3/3v/f913vqXUQZA5HJKz3fesbLObHhXjquj6j4Wt77sq06hYV1ocPsOVlHauhE+fdYEG4LgT3W9v30ZY/0+3f3AXOafe6C5I9nvpW/cPKNvi9j/mQpAoF/jqD7jpZZtd4B96qvu+j/qSC3RHWE1ogaO3BY6dq+DJS6F2P+SdDCd+DU6YQ3VMOgs/28WD721h095qThjUj+tP7c+Zn95Kv6JFBE+6iqjRM90VWnIO7P7EZXLb3nc/VA246Q1V4K9vvs/YZGiscRn0yHNdxpiRD1c+4wLYP24KuZIOMWiSywhHnO1+rFGx7mq4qZrI74PXboOCh90P218PQ093P5rEDG8j6pVW/O7Hkjet44wK3JXbq7e6zOSU70JUi9uF6ytd8Nj0hst8a/a6Kq+4ZHeMo86D+H6w9iX3o23KUGMS3Q82fajL9LJHuSqSv3/LBcu4VJehnzDHZQibF7lzdsJsFxwTjnAgqAM74YV5rjTz1YddMAql6kpqK590V79Nv+/EDPjSf7lA3aTwbVj0C3d13FSlFB3v0jj56+4CIj7FbWPDq+4KeOdKt9zQ02Hi1W7Zllff7fHVwKOzXEmk/xjYsxa++aq7Ovc3uEx+xeMuSA07w2W2J8xpv3RbW+YCTMDvAkjWyOYBVxWWPeQy6kCjO285Y+Dkb0NKTuv/71s/g0+fcZ+Tc+D0H7hSTdVul3nv2wj7vUy6eg/0P8FV++VNdd+zpv+HqiuxrnnBnf+9aw7t57gJ7jc7brYLcAf3vws+mQ9xKa700fJ/GwxC4ZvuIqV4mfu/RMW65TKHu1dMgguqTRdr317kahWOgAWO3hQ4ti+G+Ze7q+Qp34A1L8Ge1QSIYqmO5RX/NMrTx3PtxFSmZDUStfgPsL8Qzv8fV51wOLVlriSx9X33g8oc4b7UteXuyunATlcVMvYid8W77QN47uvuB99YB0mZMPNX7gq2oshV4/Qb2Ha9dVtWPuUy5+n/7qpGuuv5mqbfRMv9N9a7EkLpBvfSgAsCSZmHlvH73A964ElH1m7TmTR21f8n4HeZYNlWGDyt+fG03GfxMneVnTn8yPdXXQqPfMldIV9yn8sgQ/lqOheMIqG4wJ3jzgbG9lTtdsE8ezQMGNc122xPRZFrS5t0TesLpjBZ4DiWAse+Ta7KISnbZcJNVyS1ZbDxdfjnzZCWB19/ib1R2fz61Q2sWfkRl8YvZXb8cvo3bG++vcRMuPwxlxl3tYod8NL1LiM59+7DZzrGhDqw01XRjZ7Z3Skx7eiWwCEiM4Hf4cYcf0hVf9Vi/lDgESAHKAPmqmqxN+8bwB3eoj9X1ce86VOAR4FEYCFwk3ZwEMdE4GiocvWrKx53jY4INLVFDJjgitel6wHQ405k1Rcf5vVtQZ5csh2fP8h1Z+Zz44yRrlfZvetdCSM5x9Wn9xvUdsOuMca046gHDhGJBjYC5wHFwDLgSlVdG7LM34B/qupjInI28E1VvUZEMoECYCou91wOTFHVchFZCnwf+BgXOH6vqq+2l5YeHTg2vObqNDe+7ur2M4e7+uWJV7v69w0L3S2UsYnUDZrGEyWD+PPmDMrq3cBGM0b35z8vHEt+djcX7Y0xvU53dKs+DShU1S1eAp4BLgFCb68ZB/yH934R8JL3/svAm6pa5q37JjBTRN4B+qnqEm/648BsoN3A0WMtvs/dIpqc44LFCZfC4FMO1Uem9Ifsm9DTvs8rn+3i7gVrKK9t5JKJgzhnzADOGJVNWuIx9nCeMeaYF8nAkQuEPL1EMXBKi2U+AS7FVWfNAVJFJOsw6+Z6r+I2prciIvOAeQBDhgw54oOImM+ed0Fj7MXw1b8e9r7ryrpGfvz8J7y+Zg8TctN4/FunMG7QEd6RY4wxXaC7H0P8IfBHEbkWeA8oAQLtrhEmVX0AeABcVVVXbPOIVZbAm3e6+/+Hz3C3dL707zDkNLj0wcMGjY17qvjOE8spKqvl9vPHcN0Z+TamtjGm20UycJQAITcok+dNO0hVd+JKHIhICvAVVa0QkRLgrBbrvuOtn9dierNt9jiq8PL17hbX1X93D16Bu4/8yvmtn9oFahr8LPxsFz9dsIbk+Bienjedk4fZ3UrGmJ4hkoFjGTBKRPJxmfsVwFWhC4hINlCmqkHgdtwdVgCvA78Ukaanv74E3K6qZSJyQESm4xrHvw78IYLH8PkVPOweMJv1W9f1wJZ33UNUp94Q8nCb60PqySXbWZVyhUYAABucSURBVPjZbgq2l9EYUCYPSef+uVMY0O/zjadhjDFdKWKBQ1X9InIjLghEA4+o6hoRuQcoUNUFuFLFf4uI4qqqbvDWLROR/8IFH4B7mhrKges5dDvuq/TkhvGyrfDGXe6J6SnfdA9snXiZe4VQVX65cB0Pvr+VMcel8q3T8/nC8Tmckp9pVVPGmB7HHgCMlGAQHpvl+rG5frF7YO8w/vivTfzmjY1849Sh3H3xCb12REJjzLHlcLfj2uVsJGz/yHWpsP1D1wVHO0Hj8cXb+M0bG5kzKZefXmRBwxjT83X3XVXHpoYq119UY617aK+x1vVj1Fjr+vLZ+BqkDoSL/wgTrzrsZp5csp27Xl7DuWMH8D9fPdFG1zPGHBMscByJt++BpQ+0PS8hDc6+E6Zf327vrQ9/sJX/+udazh7Tnz9eNYlYa8swxhwjLHAcia3vuW6lL/iNu502JvHQ35j4DnstvW9RIf/v9Q2cP/44fnfFJOJiLGgYY44dFjg6q2a/62zwnLs63TVyMKj8/JV1PPLhVmZPHMRvLjvJ7poyxhxzLHB01o7F7u+Q0zq1Wn1jgJufXcWrq3dz7WnDuHPWuKMzprcxxnQxCxydtf0jN0paJ0bUqqxr5FuPLmPFjnLunDWO685oPdSrMcYcKyxwdNaOj9wQkWGOb9EYCHL9U8v5tLiC+66azAUTBkY4gcYYE1lWwd4ZDVVuPOmh4VVTqSp3vbyaDwv388s5EyxoGGN6BQscnVH0MWgw7MDx0PtbeXppETfMGMFlUwd3vIIxxhwDLHB0xvbFINGQN63DRT/YtI9fvrqOCycM5JbzRh+FxBljzNFhgaMztn8EA0+C+JR2F6tvDPCTFz8jPyuZ31x2kj0RbozpVSxwhKuxHkqWh1VNdd+iQnaU1fLzOeNJjIs+CokzxpijxwJHuHaugEBDh4GjcG8Vf353M5dOyuW0EdlHKXHGGHP0WOAI1/YP3d8hpx52EVXlP19cTVJcDD+5cOxRSpgxxhxdFjjCteE1OG4CJB1+CNdnlhXx8dYybjt/DNkp4T3nYYwxxxoLHOHYtwlKCuDErx12kcK9Vdzzj7WcPjKLr9mtt8aYXiyigUNEZorIBhEpFJHb2pg/REQWichKEflURC7wpl8tIqtCXkERmejNe8fbZtO8/pE8BgA+fRYkCiZc1ubs+sYA33t6FYlx0fz28ol2F5UxpleLWJcjIhIN3AecBxQDy0RkgaquDVnsDuA5Vb1fRMYBC4FhqvoU8JS3nQnAS6q6KmS9q1X16IwFGwzCJ8/C8BmQelybi/z6tfWs23WAR66dSv9+CUclWcYY010iWeKYBhSq6hZV9QHPAJe0WEaBft77NGBnG9u50lu3e+z4CCp3wElXtjn7/U2l/PXDbXzz9GGcPWbAUU6cMcYcfZEMHLlAUcjnYm9aqLuBuSJSjCttfK+N7XwNeLrFtL961VR3ymEG6RaReSJSICIFpaWlR3QAAHzyNMSlwJgL25z98AdbGZSWwK0zxxz5Powx5hjS3Y3jVwKPqmoecAHwhIgcTJOInALUqurqkHWuVtUJwJne65q2NqyqD6jqVFWdmpOTc2Sp89XCmpdh3Ow2h4HdW1XPextLmTM5l4RYe9DPGNM3RDJwlAChtxfledNCXQc8B6Cqi4EEIPSpuStoUdpQ1RLvbxUwH1clFhkbFoKvCk66os3ZC1btJKgwZ1JexJJgjDE9TSQDxzJglIjki0gcLggsaLHMDuAcABEZiwscpd7nKOByQto3RCRGRLK997HALGA1kfLJM5A22I0v3oa/ryjhpMHpjOzfft9VxhjTm0TsripV9YvIjcDrQDTwiKquEZF7gAJVXQDcAjwoIjfjGsqvVVX1NvEFoEhVt4RsNh543Qsa0cBbwIOROgbOug2q90JU6/i6ducB1u06wD2XnBCx3RtjTE8U0REAVXUhrtE7dNpdIe/XAm1ezqvqO8D0FtNqgCldntDDyZt62FkvriwmNlqYdeKgo5YcY4zpCbq7cfyY5A8EeWnVTmaM7k9mclx3J8cYY44qCxxH4IPCfZRWNXDpZGsUN8b0PRY4jsBrq3eTmhDDjDFHeJuvMcYcwyxwHIGlW8uYNiyT+Bh7dsMY0/dY4Oik0qoGtuyr4eT8w3evbowxvZkFjk4q2FYGwMnDLHAYY/omCxydtHRbGQmxUUzITevupBhjTLcIK3CIyAsicmFoP1J9VcG2ciYOTicups//K4wxfVS4ud+fgKuATSLyKxEZHcE09VjVDX7W7KxkmlVTGWP6sLACh6q+papXA5OBbcBbIvKRiHzT6/6jT1ixvZygYg3jxpg+Lez6FhHJAq4F/g1YCfwOF0jejEjKeqBl28qIjhImD8no7qQYY0y3CauvKhF5ERgNPAFcpKq7vFnPisjRGcK1B1i6tYwTBvUjOT6iXXwZY0yPFm4O+HtVXdTWDFU9fE+AvUiDP8CqogrmTh/a3UkxxphuFW5V1TgRSW/6ICIZInJ9hNLUI60uqaTBH7TnN4wxfV64gePbqlrR9EFVy4FvRyZJPdPSreUAnDzM2jeMMX1buIEjWkSk6YOIRAN9qj/xjXuqyE1PJCslvruTYowx3SrcNo7XcA3hf/E+f8eb1mfUNwZIirNODY0xJtwSx63AIuDfvdfbwI87WklEZorIBhEpFJHb2pg/REQWichKEflURC7wpg8TkToRWeW9/hyyzhQR+czb5u9DS0KR5PMH7WlxY4whzBKHqgaB+71XWLzqrPuA84BiYJmILPCGi21yB/Ccqt4vIuNww8wO8+ZtVtWJbWz6flz7ysfe8jOBV8NN15HyBSxwGGMMhN9X1SgReV5E1orIlqZXB6tNAwpVdYuq+oBngEtaLKNAP+99GrCzg3QMBPqp6hJVVeBxYHY4x/B5NfiDxEVb4DDGmHBzwr/irvT9wAxchv1kB+vkAkUhn4u9aaHuBuaKSDGu9PC9kHn5XhXWuyJyZsg2izvYJgAiMk9ECkSkoLS0tIOkdsyqqowxxgk3J0xU1bcBUdXtqno3cGEX7P9K4FFVzQMuAJ7weuDdBQxR1UnAfwDzRaRfO9tpRVUfUNWpqjo1J+fzD/Hq8weJt8BhjDFh31XV4GXom0TkRqAESOlgnRJgcMjnPG9aqOtwbRSo6mIRSQCyVXUv0OBNXy4im4HjvfXzOthmRFgbhzHGOOHmhDcBScD3gSnAXOAbHayzDBglIvkiEgdcASxoscwO4BwAERkLJAClIpLjNa4jIsOBUcAWr4+sAyIy3bub6uvAy2Eew+fiShx2O64xxnRY4vAy8K+p6g+BauCb4WxYVf1e6eR1IBp4RFXXiMg9QIGqLgBuAR4UkZtxDeXXqqqKyBeAe0SkEQgC31XVMm/T1wOPAom4u6kifkcVeG0c1jhujDEdBw5VDYjIGUeycVVdiGv0Dp12V8j7tcDpbaz3d+Dvh9lmATD+SNLzeTT4A1ZVZYwxhN/GsVJEFgB/A2qaJqrqCxFJVQ9kd1UZY4wTbuBIAPYDZ4dMU6DvBA5rHDfGGCD8J8fDatforYJBpTGg1sZhjDGEPwLgX3EljGZU9VtdnqIeyBcIAliJwxhjCL+q6p8h7xOAOXTQPUhv0hQ47AFAY4wJv6qq2R1OIvI08EFEUtQD+fxW4jDGmCZHmhOOAvp3ZUJ6sqbAYSUOY4wJv42jiuZtHLtxY3T0CVbiMMaYQ8KtqkqNdEJ6soamwBFtXY4YY0y443HMEZG0kM/pInJUxsHoCazEYYwxh4SbE/5UVSubPqhqBfDTyCSp5/EFAoAFDmOMgfADR1vLhXsr7zHvUFWVBQ5jjAk3JywQkf8TkRHe6/+A5ZFMWE9iVVXGGHNIuDnh9wAf8Cxu7PB64IZIJaqnsdtxjTHmkHDvqqoBbotwWnos63LEGGMOCfeuqjdFJD3kc4aIvB65ZPUsVuIwxphDws0Js707qQBQ1XL64JPjVuIwxpjwA0dQRIY0fRCRYbTRW25LIjJTRDaISKGItKrqEpEhIrJIRFaKyKcicoE3/TwRWS4in3l/zw5Z5x1vm6u8V8QDmN1VZYwxh4R7S+1/Ah+IyLuAAGcC89pbwRur/D7gPKAYWCYiC7zhYpvcATynqveLyDjcMLPDgH3ARaq6U0TG48Ytzw1Z72pvCNmjwkocxhhzSFg5oaq+BkwFNgBPA7cAdR2sNg0oVNUtqurD3Y11SctNA/2892l4XbWr6kpVbeq2fQ2QKCLx4aQ1Eqxx3BhjDgm3k8N/A24C8oBVwHRgMc2Hkm0pFygK+VwMnNJimbuBN0Tke0AycG4b2/kKsEJVG0Km/VVEAsDfgZ+raqtqMxGZh1cqGjJkSMvZnWJVVcYYc0i4OeFNwMnAdlWdAUwCKtpfJSxXAo+qah5wAfCEiBxMk4icAPwa+E7IOler6gRcddmZwDVtbVhVH1DVqao6NScn53Ml0ucPEhcdhYh8ru0YY0xvEG7gqFfVegARiVfV9cDoDtYpAQaHfM7zpoW6DngOQFUX40YXzPb2kwe8CHxdVTc3raCqJd7fKmA+rkosonz+oFVTGWOMJ9zcsNh7juMl4E0ReRnY3sE6y4BRIpIvInHAFcCCFsvsAM4BEJGxuMBR6u3rFeA2Vf2waWERiRGRpsASC8wCVod5DEfMFwhY4DDGGE+4T47P8d7eLSKLcA3Zr3Wwjl9EbsTdERUNPKKqa0TkHqBAVRfgGtkfFJGbcQ3l16qqeuuNBO4Skbu8TX4JqAFe94JGNPAW8GAnjveINFVVGWOMOYIeblX13U4suxB3i23otLtC3q8FTm9jvZ8DPz/MZqeEu/+u4vMHiY+1wGGMMXDkY473KQ1W4jDGmIMsNwyDNY4bY8whlhuGwRewwGGMMU0sNwyDVVUZY8whlhuGwaqqjDHmEMsNw+DzB20sDmOM8VhuGAZr4zDGmEMsNwyDPQBojDGHWG4YBldVFd3dyTDGmB7BAkcYGvzWV5UxxjSx3DAMdleVMcYcYrlhGKxx3BhjDrHcsAPBoNIYUGscN8YYj+WGHbDxxo0xpjnLDTvQFDjsAUBjjHEsN+yAz28lDmOMCWW5YQcOBg5r4zDGGCDCgUNEZorIBhEpFJHb2pg/REQWichKEflURC4ImXe7t94GEflyuNvsak2Bw0YANMYYJ2K5oYhEA/cB5wPjgCtFZFyLxe4AnlPVScAVwJ+8dcd5n08AZgJ/EpHoMLfZpRoOljjsyXFjjIHIljimAYWqukVVfcAzwCUtllGgn/c+Ddjpvb8EeEZVG1R1K1DobS+cbXYpa+MwxpjmIpkb5gJFIZ+LvWmh7gbmikgxsBD4XgfrhrNNAERknogUiEhBaWnpkR4DvkAAsMBhjDFNujs3vBJ4VFXzgAuAJ0SkS9Kkqg+o6lRVnZqTk3PE22mwxnFjjGkmJoLbLgEGh3zO86aFug7XhoGqLhaRBCC7g3U72maXsqoqY4xpLpK54TJglIjki0gcrrF7QYtldgDnAIjIWCABKPWWu0JE4kUkHxgFLA1zm13q4F1VFjiMMQaIYIlDVf0iciPwOhANPKKqa0TkHqBAVRcAtwAPisjNuIbya1VVgTUi8hywFvADN6hqAKCtbUbqGMC6HDHGmJYiWVWFqi7ENXqHTrsr5P1a4PTDrPsL4BfhbDOS7AFAY4xpznLDDjTYA4DGGNOM5YYdsBKHMcY0Z7lhB+yuKmOMac5yww5Y47gxxjRnuWEH7AFAY4xpznLDDvj8QeKioxCR7k6KMcb0CBY4OuDzB62ayhhjQliO2AFfIGCBwxhjQliO2IGmqipjjDGO5YgdaLCqKmOMacZyxA74/EHr4NAYY0JYjtgBaxw3xpjmLEfsgC9ggcMYY0JZjtiBBmscN8aYZixH7IBVVRljTHOWI3bAGseNMaY5yxE7YG0cxhjTXERzRBGZKSIbRKRQRG5rY/5vRWSV99ooIhXe9Bkh01eJSL2IzPbmPSoiW0PmTYzkMdgDgMYY01zEho4VkWjgPuA8oBhYJiILvOFiAVDVm0OW/x4wyZu+CJjoTc8ECoE3Qjb/I1V9PlJpD9Xgty5HjDEmVCRzxGlAoapuUVUf8AxwSTvLXwk83cb0rwKvqmptBNLYIdfGEd0duzbGmB4pkoEjFygK+VzsTWtFRIYC+cC/2ph9Ba0Dyi9E5FOvqiv+MNucJyIFIlJQWlra+dR77K4qY4xprqfkiFcAz6tqIHSiiAwEJgCvh0y+HRgDnAxkAre2tUFVfUBVp6rq1JycnCNOmDWOG2NMc5HMEUuAwSGf87xpbWmrVAFwOfCiqjY2TVDVXeo0AH/FVYlFRDCoNAbUGseNMSZEJHPEZcAoEckXkThccFjQciERGQNkAIvb2Eardg+vFIK4IflmA6u7ON0H2XjjxhjTWsTuqlJVv4jciKtmigYeUdU1InIPUKCqTUHkCuAZVdXQ9UVkGK7E8m6LTT8lIjmAAKuA70bqGJoChz0AaIwxh0QscACo6kJgYYtpd7X4fPdh1t1GG43pqnp216WwfT6/lTiMMaYlyxHbcTBwWBuHMcYcZDliOxqsxGGMMa1YjtiOphKHPQBojDGHWOBoh7VxGGNMa5YjtsMXcM8jWuAwxphDLEdsR4M1jhtjTCuWI7bDqqqMMaY1yxHbcahx3P5NxhjTxHLEdliXI8YY05rliO2wBwCNMaY1yxHbYQ8AGmNMa5YjtsPaOIwxpjXLEdthd1UZY0xrliO2wxrHjTGmNcsR22EPABpjTGuWI7bD5w8SFx2FG2zQGGMMWOBol88ftGoqY4xpIaK5oojMFJENIlIoIre1Mf+3IrLKe20UkYqQeYGQeQtCpueLyMfeNp/1xjOPCF8gYIHDGGNaiFiuKCLRwH3A+cA44EoRGRe6jKrerKoTVXUi8AfghZDZdU3zVPXikOm/Bn6rqiOBcuC6SB1DU1WVMcaYQyKZK04DClV1i6r6gGeAS9pZ/krg6fY2KK6x4WzgeW/SY8DsLkhrmxqsqsoYY1qJZK6YCxSFfC72prUiIkOBfOBfIZMTRKRARJaISFNwyAIqVNUfxjbneesXlJaWHtEB+PxBe/jPGGNaiOnuBHiuAJ5X1UDItKGqWiIiw4F/ichnQGW4G1TVB4AHAKZOnapHkihrHDfGmNYiGThKgMEhn/O8aW25ArghdIKqlnh/t4jIO8Ak4O9AuojEeKWO9rb5uU0emkFVvb/jBY0xpg+JZOBYBowSkXxc5n4FcFXLhURkDJABLA6ZlgHUqmqDiGQDpwP/o6oqIouAr+LaTL4BvBypA7hhxshIbdoYY45ZEauH8UoENwKvA+uA51R1jYjcIyKhd0ldATyjqqHVSWOBAhH5BFgE/EpV13rzbgX+Q0QKcW0eD0fqGIwxxrQmzfPr3mnq1KlaUFDQ3ckwxphjiogsV9WpLadby68xxphOscBhjDGmUyxwGGOM6RQLHMYYYzrFAocxxphOscBhjDGmU/rE7bgiUgpsP8LVs4F9XZicY0VfPO6+eMzQN4/bjjk8Q1U1p+XEPhE4Pg8RKWjrPuberi8ed188Zuibx23H/PlYVZUxxphOscBhjDGmUyxwdOyB7k5AN+mLx90Xjxn65nHbMX8O1sZhjDGmU6zEYYwxplMscBhjjOkUCxztEJGZIrJBRApF5LbuTk8kiMhgEVkkImtFZI2I3ORNzxSRN0Vkk/c3o7vT2tVEJFpEVorIP73P+SLysXe+nxWRuO5OY1cTkXQReV5E1ovIOhE5tbefaxG52fturxaRp0UkoTeeaxF5RET2isjqkGltnltxfu8d/6ciMrkz+7LAcRgiEg3cB5wPjAOuFJFx3ZuqiPADt6jqOGA6cIN3nLcBb6vqKOBt73NvcxNukLEmvwZ+q6ojgXLgum5JVWT9DnhNVccAJ+GOv9eeaxHJBb4PTFXV8UA0bvC43niuHwVmtph2uHN7PjDKe80D7u/MjixwHN40oFBVt6iqDzdU7SXdnKYup6q7VHWF974Kl5Hk4o71MW+xx4DZ3ZPCyBCRPOBC4CHvswBnA897i/TGY04DvoA3aqaq+lS1gl5+rnFDZCeKSAyQBOyiF55rVX0PKGsx+XDn9hLgcXWWAOkiMjDcfVngOLxcoCjkc7E3rdcSkWHAJOBjYICq7vJm7QYGdFOyIuVe4MdA0PucBVR4Qx5D7zzf+UAp8Feviu4hEUmmF59rVS0BfgPswAWMSmA5vf9cNzncuf1c+ZsFDgOAiKQAfwd+oKoHQud548H3mvu2RWQWsFdVl3d3Wo6yGGAycL+qTgJqaFEt1QvPdQbu6jofGAQk07o6p0/oynNrgePwSoDBIZ/zvGm9jojE4oLGU6r6gjd5T1PR1fu7t7vSFwGnAxeLyDZcFeTZuLr/dK86A3rn+S4GilX1Y+/z87hA0pvP9bnAVlUtVdVG4AXc+e/t57rJ4c7t58rfLHAc3jJglHf3RRyuQW1BN6epy3l1+w8D61T1/0JmLQC+4b3/BvDy0U5bpKjq7aqap6rDcOf1X6p6NbAI+Kq3WK86ZgBV3Q0Uichob9I5wFp68bnGVVFNF5Ek77vedMy9+lyHONy5XQB83bu7ajpQGVKl1SF7crwdInIBri48GnhEVX/RzUnqciJyBvA+8BmH6vt/gmvneA4YguuS/nJVbdnwdswTkbOAH6rqLBEZjiuBZAIrgbmq2tCd6etqIjIRd0NAHLAF+CbuArLXnmsR+RnwNdwdhCuBf8PV5/eqcy0iTwNn4bpP3wP8FHiJNs6tF0T/iKu2qwW+qaoFYe/LAocxxpjOsKoqY4wxnWKBwxhjTKdY4DDGGNMpFjiMMcZ0igUOY4wxnWKBw5geTkTOaurB15iewAKHMcaYTrHAYUwXEZG5IrJURFaJyF+88T6qReS33ngQb4tIjrfsRBFZ4o2F8GLIOAkjReQtEflERFaIyAhv8ykh42g85T3AZUy3sMBhTBcQkbG4p5NPV9WJQAC4GtepXoGqngC8i3uaF+Bx4FZVPRH31H7T9KeA+1T1JOA0XI+u4Hot/gFubJjhuP6WjOkWMR0vYowJwznAFGCZVxhIxHUoFwSe9ZZ5EnjBGxcjXVXf9aY/BvxNRFKBXFV9EUBV6wG87S1V1WLv8ypgGPBB5A/LmNYscBjTNQR4TFVvbzZR5M4Wyx1pHz+h/SgFsN+u6UZWVWVM13gb+KqI9IeDYz0Pxf3GmnphvQr4QFUrgXIROdObfg3wrjcCY7GIzPa2ES8iSUf1KIwJg121GNMFVHWtiNwBvCEiUUAjcANusKRp3ry9uHYQcF1c/9kLDE291IILIn8RkXu8bVx2FA/DmLBY77jGRJCIVKtqSnenw5iuZFVVxhhjOsVKHMYYYzrFShzGGGM6xQKHMcaYTrHAYYwxplMscBhjjOkUCxzGGGM65f8D4kK/o2CPI1gAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QR9WUYXxqtfR"
      },
      "source": [
        "# **4. 모델 저장**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Wi9yznz4qvzK",
        "colab": {}
      },
      "source": [
        "save_path = '/content/'\n",
        "team_name = 'team21'\n",
        "\n",
        "# 트레이닝된 전체 모델을 저장합니다.\n",
        "model.save(save_path +  'model_entire_'+ team_name + '.h5')"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "4aPbgI-c-Kj8"
      },
      "source": [
        "# **5. 모델 로드 및 평가**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HbhGT3VRdqwR",
        "colab_type": "text"
      },
      "source": [
        "### 전체 모델 평가"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "y7WONVxH-Kt6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "8b862c96-6556-4e31-b87f-847ed9901c4b"
      },
      "source": [
        "save_path = '/content/'\n",
        "team_name = 'team21'\n",
        "\n",
        "model = keras.models.load_model(save_path + 'model_entire_' + team_name + '.h5')\n",
        "model.evaluate(x_test_after, y_test)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "588/588 [==============================] - 3s 5ms/step - loss: 0.3174 - accuracy: 0.9049\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.3174118101596832, 0.9048936367034912]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z3IPtO37dqlE",
        "colab_type": "text"
      },
      "source": [
        "### 최고 모델 평가"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SHKyMbEKdyyI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "4b4113fb-af7e-4c9b-974b-d795c2765cfb"
      },
      "source": [
        "model = keras.models.load_model('/content/checkpoint_entire_best.h5')\n",
        "model.evaluate(x_test_after, y_test)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "588/588 [==============================] - 3s 4ms/step - loss: 0.2989 - accuracy: 0.9084\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.2988874316215515, 0.9084042310714722]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    }
  ]
}